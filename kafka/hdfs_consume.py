import json
import time
import os
import threading
from datetime import datetime
from kafka import KafkaConsumer
from hdfs import InsecureClient
import BaseConsumer
# --- C·∫§U H√åNH CHUNG ---
BOOTSTRAP_SERVERS = ['localhost:9092']
HDFS_URL = 'http://localhost:30070' # Nh·ªõ port-forward tr∆∞·ªõc khi ch·∫°y
HDFS_USER = 'root'
# =============================================================================
# T·∫¶NG 2: NH√ìM CONSUMER THEO CH·ª®C NƒÇNG
# =============================================================================

# --- NH√ìM 1: HDFS ARCHIVER (Batch Layer) ---
class HDFSConsumerBase(BaseConsumer):
    def __init__(self, topic, hdfs_folder):
        # HDFS c·∫ßn gom batch l·ªõn (v√≠ d·ª• 50 tin) m·ªõi ghi ƒë·ªÉ t·ªëi ∆∞u
        super().__init__(topic, group_id='hdfs-archiver-group', batch_size=50)
        self.hdfs_folder = hdfs_folder
        self.client = self._connect_hdfs()

    def _connect_hdfs(self):
        try:
            client = InsecureClient(HDFS_URL, user=HDFS_USER)
            client.makedirs(self.hdfs_folder) # T·∫°o folder n·∫øu ch∆∞a c√≥
            return client
        except Exception as e:
            print(f"‚ö†Ô∏è Kh√¥ng n·ªëi ƒë∆∞·ª£c HDFS: {e}")
            return None

    def process_batch(self, batch):
        if not self.client: return
        
        # T·∫°o t√™n file: topic_timestamp.json
        filename = f"{self.topic}_{int(time.time())}.json"
        full_path = os.path.join(self.hdfs_folder, filename)
        
        try:
            # Ghi file JSON Lines
            content = "\n".join([json.dumps(r) for r in batch])
            with self.client.write(full_path, encoding='utf-8') as writer:
                writer.write(content)
            print(f"üíæ [{self.__class__.__name__}] ƒê√£ ghi {len(batch)} d√≤ng v√†o {full_path}")
        except Exception as e:
            print(f"‚ùå L·ªói ghi HDFS: {e}")