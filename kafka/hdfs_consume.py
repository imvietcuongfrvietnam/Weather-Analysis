import json
import time
import os
from datetime import datetime
from kafka import KafkaConsumer
from hdfs import InsecureClient # ThÆ° viá»‡n giao tiáº¿p HDFS qua Web

# --- Cáº¤U HÃŒNH ---
KAFKA_HOST = 'localhost:9092'
TOPIC_NAME = 'nyc-weather-raw' # TÃªn topic báº¡n Ä‘Ã£ táº¡o
HDFS_URL = 'http://localhost:9870' # WebHDFS Port (Ä‘Ã£ port-forward)
HDFS_USER = 'root' # User máº·c Ä‘á»‹nh cá»§a image bde2020
HDFS_OUTPUT_DIR = '/nyc_data/weather' # ThÆ° má»¥c trÃªn HDFS

# Cáº¥u hÃ¬nh Batch (Gom nhÃ³m)
BATCH_SIZE = 50  # Cá»© 50 tin nháº¯n thÃ¬ ghi 1 file (Demo Ä‘á»ƒ tháº¥p cho nhanh tháº¥y)

def get_hdfs_client():
    """Káº¿t ná»‘i tá»›i HDFS"""
    try:
        # InsecureClient dÃ¹ng cho cá»¥m khÃ´ng cÃ³ Kerberos (máº·c Ä‘á»‹nh)
        client = InsecureClient(HDFS_URL, user=HDFS_USER)
        return client
    except Exception as e:
        print(f"âŒ KhÃ´ng káº¿t ná»‘i Ä‘Æ°á»£c HDFS: {e}")
        return None

def run_consumer():
    # 1. Káº¿t ná»‘i Kafka
    consumer = KafkaConsumer(
        TOPIC_NAME,
        bootstrap_servers=KAFKA_HOST,
        auto_offset_reset='latest',
        enable_auto_commit=True,
        value_deserializer=lambda x: json.loads(x.decode('utf-8')),
        group_id='hdfs-writer-group'
    )
    print(f"ğŸ§ Äang láº¯ng nghe Kafka topic: {TOPIC_NAME}...")

    # 2. Káº¿t ná»‘i HDFS
    hdfs_client = get_hdfs_client()
    if not hdfs_client: return

    # Táº¡o thÆ° má»¥c trÃªn HDFS náº¿u chÆ°a cÃ³
    try:
        hdfs_client.makedirs(HDFS_OUTPUT_DIR)
        print(f"ğŸ“‚ ÄÃ£ Ä‘áº£m báº£o thÆ° má»¥c tá»“n táº¡i: {HDFS_OUTPUT_DIR}")
    except:
        pass

    buffer = [] # Rá»• chá»©a tin nháº¯n táº¡m

    for message in consumer:
        data = message.value
        buffer.append(data)
        
        # In dáº¥u cháº¥m Ä‘á»ƒ biáº¿t Ä‘ang cháº¡y
        print(".", end="", flush=True)

        # 3. Kiá»ƒm tra náº¿u rá»• Ä‘áº§y thÃ¬ ghi xuá»‘ng HDFS
        if len(buffer) >= BATCH_SIZE:
            print(f"\nğŸ“¦ Äá»§ {BATCH_SIZE} tin nháº¯n. Äang ghi xuá»‘ng HDFS...")
            
            # Táº¡o tÃªn file duy nháº¥t theo thá»i gian
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"weather_{timestamp}.json"
            hdfs_path = f"{HDFS_OUTPUT_DIR}/{filename}"

            try:
                # Chuyá»ƒn list dá»¯ liá»‡u thÃ nh chuá»—i JSON (má»—i dÃ²ng 1 record)
                # Dáº¡ng nÃ y gá»i lÃ  JSON Lines, ráº¥t tá»‘t cho Spark Ä‘á»c sau nÃ y
                file_content = "\n".join([json.dumps(record) for record in buffer])
                
                # Ghi vÃ o HDFS
                # encoding='utf-8' quan trá»ng Ä‘á»ƒ ghi text
                with hdfs_client.write(hdfs_path, encoding='utf-8') as writer:
                    writer.write(file_content)
                
                print(f"âœ… ÄÃ£ ghi file: {hdfs_path}")
                
                # XÃ³a rá»• Ä‘á»ƒ gom Ä‘á»£t má»›i
                buffer = []
                
            except Exception as e:
                print(f"âŒ Lá»—i ghi file HDFS: {e}")
                # LÆ°u Ã½: Náº¿u lá»—i thá»±c táº¿ nÃªn cÃ³ cÆ¡ cháº¿ retry, á»Ÿ Ä‘Ã¢y skip Ä‘á»ƒ demo

if __name__ == "__main__":
    # Chá» 1 chÃºt Ä‘á»ƒ port-forward á»•n Ä‘á»‹nh
    print("â³ Vui lÃ²ng Ä‘áº£m báº£o báº¡n Ä‘Ã£ cháº¡y 'kubectl port-forward svc/namenode 9870:9870'")
    time.sleep(2)
    run_consumer()