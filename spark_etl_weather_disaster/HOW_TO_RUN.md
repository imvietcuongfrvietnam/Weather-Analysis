# ğŸš€ CÃ¡ch Cháº¡y Project

## ğŸ“‹ BÆ°á»›c 1: Generate Test Data (CHá»ˆ Láº¦N Äáº¦U)

**âš ï¸ Quan trá»ng:** Chá»‰ cáº§n cháº¡y **1 Láº¦N** Ä‘á»ƒ táº¡o data, hoáº·c khi muá»‘n data má»›i!

### Windows:

```bash
generate_data.bat
```

### Git Bash:

```bash
./generate_data.sh
```

Hoáº·c cháº¡y trá»±c tiáº¿p:

```bash
py -3.11 generate_data.py
```

ğŸ“ **Káº¿t quáº£:** Táº¡o folder `./data/` chá»©a:

- `weather_data.json` (1000 records)
- `311_requests.json` (500 records)
- `taxi_trips.json` (800 records)
- `collisions.json` (300 records)
- `metadata.json`

---

## âš¡ BÆ°á»›c 2: Cháº¡y ETL Pipeline

### Windows:

```bash
run.bat
```

### Git Bash (Windows):

```bash
./run.sh
```

hoáº·c

```bash
bash run.sh
```

ğŸ’¡ **Lá»£i Ã­ch:**

- KhÃ´ng cáº§n generate data má»—i láº§n cháº¡y
- Nhanh hÆ¡n 10x
- Data nháº¥t quÃ¡n cho testing
- Dá»… debug vÃ  reproduce lá»—i

---

## ğŸ“ CÃ¡ch Cháº¡y Thá»§ CÃ´ng

### Windows CMD/PowerShell:

```bash
set PYTHONIOENCODING=utf-8
py -3.11 main_etl.py
```

### Git Bash:

```bash
export PYTHONIOENCODING=utf-8
py -3.11 main_etl.py
```

---

## â“ Táº¡i Sao Pháº£i `py -3.11`?

Há»‡ thá»‘ng cÃ³ **2 phiÃªn báº£n Python**:

- âœ… **Python 3.11** - TÆ°Æ¡ng thÃ­ch vá»›i PySpark 4.0.1
- âŒ **Python 3.13** (máº·c Ä‘á»‹nh) - KhÃ´ng tÆ°Æ¡ng thÃ­ch vá»›i PySpark

Kiá»ƒm tra cÃ¡c phiÃªn báº£n Python:

```bash
py --list
```

Káº¿t quáº£:

```
 -V:3.13 *        Python 3.13 (64-bit)  â† Máº·c Ä‘á»‹nh (dáº¥u *)
 -V:3.11          Python 3.11 (64-bit)  â† PhiÃªn báº£n cáº§n dÃ¹ng
```

---

## ğŸ”§ CÃ¡ch Äáº·t Python 3.11 LÃ m Máº·c Äá»‹nh (TÃ¹y chá»n)

### CÃ¡ch 1: Chá»‰nh sá»­a thá»§ cÃ´ng

1. Má»Ÿ Windows Settings
2. VÃ o: **Apps > Apps & features > App execution aliases**
3. **Táº®T** cÃ¡c alias: `python.exe` vÃ  `python3.exe`
4. ThÃªm Python 3.11 vÃ o PATH:
   - Má»Ÿ: **System Properties > Environment Variables**
   - ThÃªm vÃ o PATH:
     ```
     C:\Users\[YOUR_USERNAME]\AppData\Local\Programs\Python\Python311
     C:\Users\[YOUR_USERNAME]\AppData\Local\Programs\Python\Python311\Scripts
     ```
   - Äáº£m báº£o Python 3.11 á»Ÿ **TRÃŠN CÃ™NG** trong danh sÃ¡ch PATH

### CÃ¡ch 2: Cháº¡y script tá»± Ä‘á»™ng

```bash
setup_python_default.bat
```

---

## ğŸ“Š Xem Káº¿t Quáº£

### Input Data (Generated Once):

```
./data/
â”œâ”€â”€ weather_data.json              â† Raw weather data (1000 records)
â”œâ”€â”€ 311_requests.json              â† Raw 311 requests (500 records)
â”œâ”€â”€ taxi_trips.json                â† Raw taxi trips (800 records)
â”œâ”€â”€ collisions.json                â† Raw collision data (300 records)
â””â”€â”€ metadata.json                  â† Data information
```

### Output Data (After ETL):

**ğŸ“ Cleaned Data (JSON format - for inspection):**

```
./output/
â”œâ”€â”€ weather_cleaned.json           â† Cleaned weather data (JSON readable)
â”œâ”€â”€ 311_requests_cleaned.json      â† Cleaned 311 requests
â”œâ”€â”€ taxi_trips_cleaned.json        â† Cleaned taxi trips
â”œâ”€â”€ collisions_cleaned.json        â† Cleaned collision data
â””â”€â”€ integrated_final.json          â† Final enriched & integrated data
```

**ğŸ“ Legacy Output (CSV/Parquet - for compatibility):**

```
./fake_output/
â”œâ”€â”€ stage_1_cleaned_weather/       â† Data sau bÆ°á»›c CLEAN
â”œâ”€â”€ stage_2_normalized_weather/    â† Data sau bÆ°á»›c NORMALIZE
â”œâ”€â”€ stage_3_enriched_weather/      â† Data sau bÆ°á»›c ENRICH
â””â”€â”€ weather_disaster_integrated/   â† Data cuá»‘i cÃ¹ng (38 features)
```

**ğŸ’¡ Xem cleaned data:**

```bash
# Windows Explorer
explorer output

# Git Bash
start output

# Hoáº·c dÃ¹ng text editor
code output/weather_cleaned.json
```

**ğŸ” So sÃ¡nh INPUT vs OUTPUT:**

- **INPUT** (`./data/*.csv`): Raw data tá»« generate_data.py
- **OUTPUT** (`./output/*.json`): Cleaned data vá»›i type casting, validation, null handling

---

## ğŸ¯ Kiá»ƒm Tra Nhanh

Chá»‰ muá»‘n test xem Python vÃ  PySpark hoáº¡t Ä‘á»™ng khÃ´ng?

```bash
py -3.11 test_simple.py
```

---

## ğŸ“¦ Dependencies

```bash
# CÃ i Ä‘áº·t dependencies (náº¿u chÆ°a cÃ³)
py -3.11 -m pip install -r requirements.txt
```

**requirements.txt** bao gá»“m:

- pyspark==4.0.1
- pandas>=2.0.0
- numpy>=1.24.0
- pyarrow>=10.0.0

---

## ğŸ”„ Chuyá»ƒn Sang Kafka/HDFS/Elasticsearch (TÆ°Æ¡ng Lai)

### ğŸ“¥ INPUT: Chuyá»ƒn tá»« CSV sang Kafka

**Hiá»‡n táº¡i:** CSV files (batch) â†’ Dá»… test, khÃ´ng cáº§n setup
**TÆ°Æ¡ng lai:** Kafka streams (real-time) â†’ Production-ready

**File:** `main_etl.py`

```python
# Thay Ä‘á»•i tá»«:
reader = DataReader(spark, source_type="json")

# Sang:
reader = DataReader(spark, source_type="kafka")
```

**Kafka Configuration (Khi ready):**

```python
# In readers/real_data_reader.py
df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "weather-topic") \
    .option("startingOffsets", "earliest") \
    .load()
```

**ğŸ“ Topics cáº§n thiáº¿t:**

- `weather-topic` - Weather data stream
- `311-topic` - 311 requests stream
- `taxi-topic` - Taxi trips stream
- `collision-topic` - Collision data stream

---

### ğŸ’¾ OUTPUT: Chuyá»ƒn tá»« JSON sang HDFS/Elasticsearch

**Hiá»‡n táº¡i:** JSON files â†’ Readable, dá»… inspect
**TÆ°Æ¡ng lai:** HDFS + Elasticsearch â†’ Scalable storage + search

**File:** `main_etl.py`

```python
# Thay Ä‘á»•i tá»«:
data_writer = DataWriter(output_type="json")

# Sang HDFS:
data_writer = DataWriter(output_type="hdfs")

# Hoáº·c Elasticsearch:
data_writer = DataWriter(output_type="elasticsearch")
```

**HDFS Configuration (Khi ready):**

```python
# In writers/real_data_writer.py
df.write \
    .mode("overwrite") \
    .format("parquet") \
    .partitionBy("date") \
    .save("hdfs://namenode:9000/data/weather_cleaned")
```

**Elasticsearch Configuration (Khi ready):**

```python
# In writers/real_data_writer.py
df.write \
    .format("org.elasticsearch.spark.sql") \
    .option("es.nodes", "localhost") \
    .option("es.port", "9200") \
    .option("es.resource", "weather-disaster-nyc") \
    .mode("append") \
    .save()
```

**ğŸ’¡ Architecture Flow:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CURRENT (Development)                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  CSV Files â†’ Spark ETL â†’ JSON Files                        â”‚
â”‚  (./data/)              (./output/)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FUTURE (Production)                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Kafka Streams â†’ Spark ETL â†’ HDFS (storage)               â”‚
â”‚                              â†’ Elasticsearch (search/viz)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
