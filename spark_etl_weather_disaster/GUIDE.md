# Spark ETL - HÆ°á»›ng dáº«n sá»­ dá»¥ng

## ğŸ¯ Giá»›i thiá»‡u

ÄÃ¢y lÃ  pháº§n **Spark ETL Batch** cho project "PhÃ¢n tÃ­ch dá»¯ liá»‡u thá»i tiáº¿t dá»± bÃ¡o thiÃªn tai - NYC"

### Chá»©c nÄƒng:

- âœ… Äá»c data tá»« 4 nguá»“n: Weather, 311 Requests, Taxi Trips, Collisions
- âœ… Clean data (remove nulls, duplicates, outliers)
- âœ… Normalize (chuáº©n hÃ³a units, formats)
- âœ… Enrich (tÃ­nh disaster risk, traffic impact, ML features)
- âœ… Write to HDFS + Elasticsearch

### Hiá»‡n táº¡i:

- âœ… Code hoÃ n chá»‰nh vá»›i **FAKE I/O**
- âœ… CÃ³ thá»ƒ cháº¡y standalone Ä‘á»ƒ test
- â³ ChÆ°a tÃ­ch há»£p Kafka/HDFS/ES tháº­t (sáº½ lÃ m sau)

---

## ğŸ“‚ Cáº¥u trÃºc Project

```
spark_etl_weather_disaster/
â”œâ”€â”€ README.md                  # File nÃ y
â”œâ”€â”€ GUIDE.md                   # HÆ°á»›ng dáº«n chi tiáº¿t
â”œâ”€â”€ main_etl.py                # Main pipeline (CHáº Y FILE NÃ€Y)
â”‚
â”œâ”€â”€ schemas/                   # Data schemas
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ data_schemas.py        # 4 source schemas + processed schema
â”‚
â”œâ”€â”€ readers/                   # Data readers
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ data_readers.py        # FakeDataReader + RealDataReader template
â”‚
â”œâ”€â”€ transformations/           # ETL transformations
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cleaning.py            # Clean functions
â”‚   â”œâ”€â”€ normalization.py       # Normalize functions
â”‚   â””â”€â”€ enrichment.py          # Enrich functions
â”‚
â”œâ”€â”€ writers/                   # Data writers
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ data_writers.py        # FakeDataWriter + RealDataWriter template
â”‚
â””â”€â”€ fake_output/               # Output folder (táº¡o tá»± Ä‘á»™ng)
```

---

## ğŸš€ CÃ¡ch cháº¡y

### YÃªu cáº§u:

```bash
# Install PySpark
pip install pyspark

# Optional
pip install pandas numpy
```

### Cháº¡y ETL Pipeline:

```bash
cd spark_etl_weather_disaster
python main_etl.py
```

### Output:

```
ğŸš€ SPARK ETL - WEATHER & DISASTER PREDICTION - NYC
================================================================================

ğŸ“– STEP 1: READING DATA FROM 4 SOURCES
ğŸ“Š [FAKE] Generating 1000 weather records...
   âœ… Generated 1000 weather records
ğŸ“Š [FAKE] Generating 500 311 service requests...
   âœ… Generated 500 311 requests
...

ğŸ§¹ STEP 2: CLEANING DATA
...

ğŸ“ STEP 3: NORMALIZING DATA
...

âœ¨ STEP 4: ENRICHING DATA
...

ğŸ’¾ STEP 5: WRITING DATA
...

âœ… ETL PIPELINE COMPLETED SUCCESSFULLY!
```

---

## ğŸ“Š Dá»¯ liá»‡u Ä‘Æ°á»£c xá»­ lÃ½

### 1. Weather Data (Kaggle)

- Temperature, humidity, pressure, wind
- Precipitation (rain + snow)
- Weather conditions

### 2. 311 Service Requests (NYC)

- Complaints/requests (tree damage, flooding, etc.)
- Location, borough
- Response time

### 3. Taxi Trips (NYC TLC)

- Pickup/dropoff locations & times
- Trip distance, duration, speed
- Fare information

### 4. Motor Vehicle Collisions (NYC)

- Crash location, time
- Injuries, fatalities
- Contributing factors (weather-related)

---

## ğŸ”„ Quy trÃ¬nh ETL

```
READ (Fake)
    â†“
CLEAN (Remove nulls, duplicates, outliers)
    â†“
NORMALIZE (Standardize units, formats)
    â†“
ENRICH (Calculate risk scores, ML features)
    â†“
INTEGRATE (Join 4 sources by time/location)
    â†“
WRITE (Fake HDFS + Fake ES)
```

---

## ğŸ“ˆ Features Ä‘Æ°á»£c tÃ­nh toÃ¡n

### Disaster Risk Score (0-100)

- Precipitation risk
- Wind speed risk
- Temperature extremes
- Low pressure (storms)
- Weather severity

### Traffic Impact Score (0-100)

- Trip count reduction
- Collision increase
- Casualties

### ML Features

- Time-based: hour, day_of_week, season, is_weekend, is_rush_hour
- Weather comfort index
- 24h rolling averages
- Weather-traffic correlations

---

## ğŸ”§ TÃ­ch há»£p vá»›i Pipeline tháº­t

### BÆ°á»›c 1: Thay Fake Readers

```python
# Trong main_etl.py, thay:
from readers.data_readers import FakeDataReader
reader = FakeDataReader(spark)

# ThÃ nh:
from readers.data_readers import RealDataReader
reader = RealDataReader(spark)

# VÃ  trong RealDataReader, implement:
def read_from_kafka(self, topic):
    return self.spark.readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", "your-kafka:9092") \
        .option("subscribe", topic) \
        .load()
```

### BÆ°á»›c 2: Thay Fake Writers

```python
# Thay:
from writers.data_writers import FakeDataWriter
writer = FakeDataWriter()

# ThÃ nh:
from writers.data_writers import RealDataWriter
writer = RealDataWriter()

# VÃ  implement write_to_hdfs(), write_to_elasticsearch()
```

### BÆ°á»›c 3: Config

- Kafka bootstrap servers
- HDFS namenode address
- Elasticsearch nodes/port

---

## ğŸ“ TODO List

- [ ] Integrate real Kafka readers
- [ ] Integrate real HDFS writers
- [ ] Integrate real Elasticsearch writers
- [ ] Add error handling & retry logic
- [ ] Add comprehensive logging
- [ ] Add unit tests
- [ ] Add data quality checks
- [ ] Optimize performance (partitioning, caching)
- [ ] Deploy to Spark cluster
- [ ] Schedule with Airflow

---

## ğŸ’¡ Tips

1. **Testing**: Cháº¡y vá»›i fake data trÆ°á»›c Ä‘á»ƒ test logic
2. **Debugging**: Check Spark UI (localhost:4040 khi cháº¡y)
3. **Performance**: Sá»­ dá»¥ng `.cache()` cho data Ä‘Æ°á»£c reuse nhiá»u láº§n
4. **Partitioning**: Partition by date/hour khi write HDFS

---

## ğŸ“ Káº¿t quáº£ há»c Ä‘Æ°á»£c

Sau khi hoÃ n thÃ nh pháº§n nÃ y, báº¡n sáº½ hiá»ƒu:

- âœ… Spark ETL pipeline design
- âœ… Data cleaning, normalization, enrichment
- âœ… Multi-source data integration
- âœ… Feature engineering cho ML
- âœ… Modular code organization
- âœ… Fake I/O cho testing trÆ°á»›c khi deploy

---

## ğŸ“ Support

Náº¿u cÃ³ váº¥n Ä‘á», check:

1. Spark logs
2. Python traceback
3. Data schema mismatches
4. Null pointer errors

---

**Good luck vá»›i project! ğŸš€**
