"""
MAIN ETL PIPELINE - LAMBDA ARCHITECTURE
Spark ETL Streaming: 
  1. Batch Layer: Kafka -> MinIO (Parquet) - LÆ°u trá»¯ dÃ i háº¡n
  2. Speed Layer: Kafka -> Redis (Key-Value) - Realtime Dashboard

CHáº Y: python main_etl.py
"""

from pyspark.sql import SparkSession
import sys
import os

# Cáº¥u hÃ¬nh mÃ´i trÆ°á»ng
os.environ['PYSPARK_PYTHON'] = sys.executable
os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# Import cÃ¡c modules
import kafka_config
import minio_config
# import redis_config (KhÃ´ng báº¯t buá»™c import á»Ÿ Ä‘Ã¢y náº¿u bÃªn writer Ä‘Ã£ import)

from readers.real_data_reader import DataReader
from transformations.cleaning import DataCleaner
from transformations.normalization import DataNormalizer
from transformations.enrichment import DataEnricher

# --- Má»šI: Import Redis Writer ---
from writers.redis_data_writer import RedisWriter 

def create_spark_session():
    """Khá»Ÿi táº¡o Spark vá»›i há»— trá»£ S3/MinIO"""
    packages = [
        "org.apache.hadoop:hadoop-aws:3.3.4",
        "com.amazonaws:aws-java-sdk-bundle:1.12.262"
    ]
    
    builder = SparkSession.builder \
        .appName("WeatherLambdaArchitecture") \
        .master("local[*]") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.driver.memory", "2g") \
        .config("spark.jars.packages", ",".join(packages))

    print("ğŸ“¦ Äang náº¡p cáº¥u hÃ¬nh MinIO S3...")
    for key, value in minio_config.SPARK_S3_CONFIG.items():
        builder = builder.config(key, value)
    
    spark = builder.getOrCreate()
    spark.sparkContext.setLogLevel("WARN")
    return spark

def run_etl_pipeline():
    print("\n" + "="*80)
    print("ğŸš€ SPARK LAMBDA: KAFKA -> MINIO & REDIS")
    print("="*80)
    
    spark = create_spark_session()
    
    # 1. READ & TRANSFORM (DÃ¹ng chung cho cáº£ 2 nhÃ¡nh)
    reader = DataReader(spark, source_type="kafka", kafka_mode="streaming")
    cleaner = DataCleaner()
    normalizer = DataNormalizer()
    enricher = DataEnricher()
    
    print(f"\nğŸ§ Äang Ä‘á»c topic '{kafka_config.KAFKA_TOPICS['weather']}'...")
    weather_df = reader.read_weather_data()
    
    print("âš™ï¸  Äang xá»­ lÃ½ dá»¯ liá»‡u...")
    weather_clean = cleaner.clean_weather_data(weather_df)
    weather_norm = normalizer.normalize_weather_data(weather_clean)
    weather_enriched = enricher.enrich_with_disaster_risk(weather_norm)
    
    # ==========================================
    # NHÃNH 1: BATCH LAYER -> MINIO (LÆ°u kho)
    # ==========================================
    output_minio = minio_config.get_minio_path("enriched", "weather", format="parquet")
    # Checkpoint riÃªng cho MinIO
    ckpt_minio = f"s3a://{minio_config.MINIO_BUCKET}/checkpoints/weather_minio"
    
    print(f"\nğŸ’¾ Cáº¥u hÃ¬nh MinIO (Batch Layer - 30s):")
    print(f"   ğŸ‘‰ Path: {output_minio}")
    
    query_minio = weather_enriched.writeStream \
        .outputMode("append") \
        .format("parquet") \
        .option("path", output_minio) \
        .option("checkpointLocation", ckpt_minio) \
        .trigger(processingTime="30 seconds") \
        .queryName("WriteToMinIO") \
        .start()

    # ==========================================
    # NHÃNH 2: SPEED LAYER -> REDIS (Realtime)
    # ==========================================
    # Khá»Ÿi táº¡o Writer
    redis_writer = RedisWriter()
    # Checkpoint riÃªng cho Redis (QUAN TRá»ŒNG: KhÃ´ng Ä‘Æ°á»£c trÃ¹ng vá»›i MinIO)
    ckpt_redis = f"s3a://{minio_config.MINIO_BUCKET}/checkpoints/weather_redis"
    
    print(f"\nğŸ”¥ Cáº¥u hÃ¬nh Redis (Speed Layer - 5s):")
    print(f"   ğŸ‘‰ Checkpoint: {ckpt_redis}")
    
    query_redis = weather_enriched.writeStream \
        .outputMode("append") \
        .foreachBatch(redis_writer.write_stream_to_redis) \
        .option("checkpointLocation", ckpt_redis) \
        .trigger(processingTime="5 seconds") \
        .queryName("WriteToRedis") \
        .start()

    print("\n" + "="*80)
    print("âœ… PIPELINE ÄA LUá»’NG ÄANG CHáº Y!")
    print("   1. MinIO: Ghi má»—i 30 giÃ¢y (LÆ°u lá»‹ch sá»­)")
    print("   2. Redis: Ghi má»—i 5 giÃ¢y (Cáº­p nháº­t Dashboard)")
    print("ğŸ‘‰ Nháº¥n Ctrl+C Ä‘á»ƒ dá»«ng láº¡i.")
    print("="*80)

    try:
        # Chá» Báº¤T Ká»² query nÃ o káº¿t thÃºc (hoáº·c lá»—i)
        spark.streams.awaitAnyTermination()
    except KeyboardInterrupt:
        print("\nğŸ›‘ Äang dá»«ng pipeline...")
        # Dá»«ng tá»«ng query má»™t cÃ¡ch an toÃ n
        if query_minio.isActive: query_minio.stop()
        if query_redis.isActive: query_redis.stop()
        spark.stop()
        print("âœ… ÄÃ£ dá»«ng thÃ nh cÃ´ng.")

if __name__ == "__main__":
    try:
        run_etl_pipeline()
    except Exception as e:
        print(f"\nâŒ Lá»–I: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)