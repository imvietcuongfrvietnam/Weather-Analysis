"""
MAIN ETL PIPELINE
Spark ETL Streaming: Kafka -> Spark -> MinIO (S3)

CHáº Y: python main_etl.py
"""

from pyspark.sql import SparkSession
import sys
import os

# Cáº¥u hÃ¬nh mÃ´i trÆ°á»ng
os.environ['PYSPARK_PYTHON'] = sys.executable
os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# Import cÃ¡c modules
import kafka_config
import minio_config  # <--- QUAN TRá»ŒNG: Pháº£i cÃ³ cÃ¡i nÃ y má»›i káº¿t ná»‘i Ä‘Æ°á»£c
from readers.real_data_reader import DataReader
from transformations.cleaning import DataCleaner
from transformations.normalization import DataNormalizer
from transformations.enrichment import DataEnricher

def create_spark_session():
    """Khá»Ÿi táº¡o Spark vá»›i há»— trá»£ S3/MinIO"""
    
    # 1. Äá»‹nh nghÄ©a thÆ° viá»‡n AWS Ä‘á»ƒ Spark nÃ³i chuyá»‡n Ä‘Æ°á»£c vá»›i MinIO
    packages = [
        "org.apache.hadoop:hadoop-aws:3.3.4",
        "com.amazonaws:aws-java-sdk-bundle:1.12.262"
    ]
    
    builder = SparkSession.builder \
        .appName("WeatherStreamingToMinIO") \
        .master("local[*]") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.driver.memory", "2g") \
        .config("spark.jars.packages", ",".join(packages)) # <--- Táº£i thÆ° viá»‡n AWS

    # 2. Náº¡p cáº¥u hÃ¬nh MinIO tá»« file config
    print("ğŸ“¦ Äang náº¡p cáº¥u hÃ¬nh MinIO S3...")
    for key, value in minio_config.SPARK_S3_CONFIG.items():
        builder = builder.config(key, value)
    
    spark = builder.getOrCreate()
    spark.sparkContext.setLogLevel("WARN")
    return spark

def run_etl_pipeline():
    print("\n" + "="*80)
    print("ğŸš€ SPARK STREAMING: KAFKA -> MINIO (DATA LAKE)")
    print("="*80)
    
    # 1. KHá»I Táº O
    spark = create_spark_session()
    
    reader = DataReader(spark, source_type="kafka", kafka_mode="streaming")
    cleaner = DataCleaner()
    normalizer = DataNormalizer()
    enricher = DataEnricher()
    
    # 2. Äá»ŒC Dá»® LIá»†U Tá»ª KAFKA
    print(f"\nğŸ§ Äang Ä‘á»c topic '{kafka_config.KAFKA_TOPICS['weather']}'...")
    weather_df = reader.read_weather_data()
    
    # 3. Xá»¬ LÃ Dá»® LIá»†U
    print("âš™ï¸  Äang xá»­ lÃ½ dá»¯ liá»‡u...")
    weather_clean = cleaner.clean_weather_data(weather_df)
    weather_norm = normalizer.normalize_weather_data(weather_clean)
    weather_enriched = enricher.enrich_with_disaster_risk(weather_norm)
    
    # 4. Cáº¤U HÃŒNH ÄÆ¯á»œNG DáºªN MINIO
    # Láº¥y Ä‘Æ°á»ng dáº«n tá»« file minio_config
    output_path = minio_config.get_minio_path("enriched", "weather", format="parquet")
    
    # Checkpoint lÃ  Báº®T BUá»˜C khi ghi xuá»‘ng MinIO
    checkpoint_path = f"s3a://{minio_config.MINIO_BUCKET}/checkpoints/weather"
    
    print(f"\nğŸ’¾ Cáº¥u hÃ¬nh lÆ°u trá»¯:")
    print(f"   ğŸ‘‰ Output Path:     {output_path}")
    print(f"   ğŸ‘‰ Checkpoint Path: {checkpoint_path}")
    
    # 5. GHI Dá»® LIá»†U XUá»NG MINIO
    # Chuyá»ƒn format tá»« console sang parquet
    query = weather_enriched.writeStream \
        .outputMode("append") \
        .format("parquet") \
        .option("path", output_path) \
        .option("checkpointLocation", checkpoint_path) \
        .trigger(processingTime="30 seconds") \
        .queryName("WeatherToMinIO") \
        .start()

    print("\n" + "="*80)
    print("âœ… PIPELINE ÄANG CHáº Y NGáº¦M!")
    print("ğŸ‘‰ Dá»¯ liá»‡u Ä‘ang Ä‘Æ°á»£c Ä‘áº©y vÃ o MinIO má»—i 30 giÃ¢y.")
    print("ğŸ‘‰ Kiá»ƒm tra táº¡i: http://localhost:9001/browser/weather-data")
    print("ğŸ‘‰ Nháº¥n Ctrl+C Ä‘á»ƒ dá»«ng láº¡i.")
    print("="*80)

    try:
        query.awaitTermination()
    except KeyboardInterrupt:
        print("\nğŸ›‘ Äang dá»«ng pipeline...")
        query.stop()
        spark.stop()
        print("âœ… ÄÃ£ dá»«ng thÃ nh cÃ´ng.")

if __name__ == "__main__":
    try:
        run_etl_pipeline()
    except Exception as e:
        print(f"\nâŒ Lá»–I: {e}")
        if "ClassNotFoundException" in str(e) or "hadoop-aws" in str(e):
            print("\nğŸ’¡ Gá»¢I Ã: Lá»—i nÃ y do thiáº¿u thÆ° viá»‡n hadoop-aws. HÃ£y kiá»ƒm tra káº¿t ná»‘i internet Ä‘á»ƒ Spark táº£i vá».")
        import traceback
        traceback.print_exc()
        sys.exit(1)