from pyspark.sql import SparkSession
from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType, StructType, DoubleType, IntegerType
from pyspark.ml.feature import VectorAssembler, StringIndexer, StandardScaler
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml import Pipeline
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
import random
import numpy as np

# --- C·∫§U H√åNH & CHU·∫®N B·ªä D·ªÆ LI·ªÜU GI·∫¢ L·∫¨P ---
DATASET_SIZE = 1000  # K√≠ch th∆∞·ªõc d·ªØ li·ªáu gi·∫£ l·∫≠p ƒë·ªÉ Training
WEATHER_OPTS = ['light rain', 'sky is clear', 'fog', 'thunderstorm', 'broken clouds', 'mist']

# 1. KH·ªûI T·∫†O SPARK SESSION
spark = SparkSession.builder \
    .appName("WeatherClassifierMockTraining") \
    .master("local[*]") \
    .getOrCreate()
    
spark.sparkContext.setLogLevel("WARN")
print("‚úÖ Spark Session ƒë√£ kh·ªüi ƒë·ªông.")

# H√†m t·∫°o d·ªØ li·ªáu gi·∫£ l·∫≠p l·ªõn (ƒë·ªÉ thay th·∫ø HDFS)
def generate_mock_data(size):
    data = []
    for _ in range(size):
        # T·∫°o d·ªØ li·ªáu ng·∫´u nhi√™n v·ªõi ph√¢n ph·ªëi h∆°i nghi√™ng v·ªÅ c√°c tr∆∞·ªùng h·ª£p th·ª±c t·∫ø
        desc = random.choice(WEATHER_OPTS)
        
        if 'rain' in desc or 'thunderstorm' in desc: # Tr∆∞·ªùng h·ª£p M∆∞a/B√£o
            temp = random.uniform(5, 20)
            humidity = random.uniform(80, 100)
            wind = random.uniform(5, 15)
        elif 'clear' in desc: # Tr∆∞·ªùng h·ª£p Tr·ªùi quang
            temp = random.uniform(15, 30)
            humidity = random.uniform(30, 60)
            wind = random.uniform(0, 5)
        else: # Tr∆∞·ªùng h·ª£p M√¢y/S∆∞∆°ng
            temp = random.uniform(10, 25)
            humidity = random.uniform(60, 90)
            wind = random.uniform(2, 10)
            
        data.append({
            'temperature': round(temp, 2),
            'humidity': round(humidity, 2),
            'pressure': round(random.uniform(1000, 1025), 2),
            'wind_direction': random.randint(0, 360),
            'wind_speed': round(wind, 2),
            'weather_desc': desc
        })
    return spark.createDataFrame(data)

# ƒê·ªçc d·ªØ li·ªáu (B∆Ø·ªöC N√ÄY THAY TH·∫æ CHO spark.read.csv(HDFS_INPUT_PATH))
df_raw = generate_mock_data(DATASET_SIZE)

# --- 2. LOGIC NGHI·ªÜP V·ª§ (H√†m Gom nh√≥m c·ªßa b·∫°n) ---
def define_weather_group(desc):
    desc = str(desc).lower()
    
    precipitation_keywords = ['rain', 'snow', 'thunderstorm', 'drizzle', 'sleet', 'squall', 'shower']
    if any(x in desc for x in precipitation_keywords):
        return 'Precipitation'
    
    cloudy_keywords = ['cloud', 'fog', 'mist', 'haze', 'smoke', 'dust', 'sand', 'ash']
    if any(x in desc for x in cloudy_keywords):
        return 'Cloudy/Fog'
    
    elif 'clear' in desc:
        return 'Clear'
    
    else:
        return 'Cloudy/Fog'

# ƒêƒÉng k√Ω h√†m gom nh√≥m th√†nh UDF (User Defined Function)
define_weather_group_udf = udf(define_weather_group, StringType())


# --- 3. CHUY·ªÇN ƒê·ªîI (TRANSFORMATION) ---
df = df_raw.withColumn('weather_group', define_weather_group_udf(col('weather_desc')))

# Lo·∫°i b·ªè c√°c h√†ng c√≥ gi√° tr·ªã null (ƒë·ªÉ code kh√¥ng l·ªói)
df = df.dropna()


# --- B∆Ø·ªöC 4: X√ÇY D·ª∞NG PIPELINE ML ---

# A. Chuy·ªÉn ƒë·ªïi nh√£n chu·ªói th√†nh s·ªë (StringIndexer)
indexer = StringIndexer(inputCol="weather_group", outputCol="label")

# B. Gom c√°c Features l·∫°i th√†nh 1 Vector (VectorAssembler)
features = ['temperature', 'humidity', 'pressure', 'wind_direction', 'wind_speed']
assembler = VectorAssembler(inputCols=features, outputCol="features_unscaled")

# C. Chu·∫©n h√≥a d·ªØ li·ªáu (StandardScaler)
scaler = StandardScaler(inputCol="features_unscaled", outputCol="features",
                        withStd=True, withMean=False)

# D. M√¥ h√¨nh Ph√¢n lo·∫°i (RandomForestClassifier)
rf = RandomForestClassifier(labelCol="label", featuresCol="features",
                            numTrees=100, maxDepth=20, 
                            seed=42)

# E. ƒê·ªãnh nghƒ©a c√°c b∆∞·ªõc trong Pipeline
pipeline = Pipeline(stages=[assembler, scaler, indexer, rf])


# --- 5. HU·∫§N LUY·ªÜN V√Ä ƒê√ÅNH GI√Å ---

# Chia Train/Test 
(trainingData, testData) = df.randomSplit([0.85, 0.15], seed=42)
print("üöÄ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán m√¥ h√¨nh Random Forest tr√™n Spark (Local Mode)...")

# Hu·∫•n luy·ªán m√¥ h√¨nh
model = pipeline.fit(trainingData)
print("‚úÖ Hu·∫•n luy·ªán ho√†n t·∫•t!")

# D·ª± ƒëo√°n tr√™n t·∫≠p Test
predictions = model.transform(testData)

# ƒê√°nh gi√°
evaluator = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="f1")
f1_score = evaluator.evaluate(predictions)
accuracy = evaluator.evaluate(predictions, {evaluator.metricName: "accuracy"})

print("\n--- K·∫æT QU·∫¢ ƒê√ÅNH GI√Å TR√äN SPARK ---")
print(f"ƒê·ªô ch√≠nh x√°c (Accuracy): {accuracy:.4f}")
print(f"F1 Score (ƒê·ªô c√¢n b·∫±ng): {f1_score:.4f}")


# --- 6. (B·ªé QUA HDFS) Ch·ªâ c·∫ßn d·ª´ng Session ---
# B∆Ø·ªöC N√ÄY KH√îNG C√íN L∆ØU V√ÄO HDFS N·ªÆA V√å TA ƒêANG DEBUG L·ªñI HDFS
# Sau khi s·ª≠a xong l·ªói HDFS, b·∫°n s·∫Ω b·∫≠t l·∫°i ƒëo·∫°n code l∆∞u model.
print("\nüí° ƒê√£ ho√†n t·∫•t ki·ªÉm tra logic ML. D·ª´ng Spark Session.")
spark.stop()