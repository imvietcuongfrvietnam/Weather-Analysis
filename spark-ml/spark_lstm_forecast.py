"""
Spark ML vá»›i LSTM Ä‘á»ƒ dá»± Ä‘oÃ¡n thá»i tiáº¿t 7 ngÃ y tá»›i
Äá»c dá»¯ liá»‡u tá»« MinIO, huáº¥n luyá»‡n LSTM, vÃ  ghi káº¿t quáº£ vÃ o PostgreSQL

CHáº Y:
    python spark_lstm_forecast.py
    hoáº·c
    spark-submit --packages org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.postgresql:postgresql:42.5.0 spark_lstm_forecast.py
"""

import sys
import os
from datetime import datetime, timedelta
import numpy as np
import pandas as pd

# ThÃªm Ä‘Æ°á»ng dáº«n Ä‘á»ƒ import config
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'spark_etl_weather_disaster'))

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, window, count, avg, max as spark_max, min as spark_min
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType, IntegerType
from pyspark.sql import functions as F

# Import configs
import minio_config
import postgres_config

# TensorFlow/Keras cho LSTM
try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import LSTM, Dense, Dropout
    from tensorflow.keras.optimizers import Adam
    from sklearn.preprocessing import MinMaxScaler
    TF_AVAILABLE = True
except ImportError:
    print("âš ï¸  TensorFlow khÃ´ng Ä‘Æ°á»£c cÃ i Ä‘áº·t. CÃ i Ä‘áº·t báº±ng: pip install tensorflow")
    TF_AVAILABLE = False


# ===========================
# Cáº¤U HÃŒNH
# ===========================

# Sá»‘ ngÃ y Ä‘á»ƒ dá»± Ä‘oÃ¡n
FORECAST_DAYS = 7

# Sá»‘ ngÃ y lá»‹ch sá»­ Ä‘á»ƒ huáº¥n luyá»‡n (window size)
LOOKBACK_DAYS = 30

# CÃ¡c features Ä‘á»ƒ dá»± Ä‘oÃ¡n
FEATURE_COLUMNS = ['temperature', 'humidity', 'pressure', 'wind_speed', 'wind_direction']

# Batch size cho LSTM
BATCH_SIZE = 32
EPOCHS = 50


# ===========================
# KHá»I Táº O SPARK SESSION
# ===========================

def create_spark_session():
    """Khá»Ÿi táº¡o Spark vá»›i há»— trá»£ S3/MinIO vÃ  PostgreSQL"""
    packages = [
        "org.apache.hadoop:hadoop-aws:3.3.4",
        "com.amazonaws:aws-java-sdk-bundle:1.12.262",
        "org.postgresql:postgresql:42.5.0"
    ]
    
    builder = SparkSession.builder \
        .appName("WeatherLSTMForecast") \
        .master("local[*]") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.driver.memory", "4g") \
        .config("spark.executor.memory", "4g") \
        .config("spark.jars.packages", ",".join(packages))

    # Cáº¥u hÃ¬nh MinIO S3
    print("ğŸ“¦ Äang náº¡p cáº¥u hÃ¬nh MinIO S3...")
    for key, value in minio_config.SPARK_S3_CONFIG.items():
        builder = builder.config(key, value)
    
    spark = builder.getOrCreate()
    spark.sparkContext.setLogLevel("WARN")
    return spark


# ===========================
# Äá»ŒC Dá»® LIá»†U Tá»ª MINIO
# ===========================

def read_data_from_minio(spark):
    """
    Äá»c dá»¯ liá»‡u thá»i tiáº¿t tá»« MinIO
    
    Returns:
        DataFrame: Spark DataFrame chá»©a dá»¯ liá»‡u thá»i tiáº¿t
    """
    print("\n" + "="*80)
    print("ğŸ“¥ Äá»ŒC Dá»® LIá»†U Tá»ª MINIO")
    print("="*80)
    
    # ÄÆ°á»ng dáº«n Ä‘áº¿n dá»¯ liá»‡u enriched trong MinIO
    minio_path = minio_config.get_minio_path("enriched", "weather", format="parquet")
    
    print(f"ğŸ“ ÄÆ°á»ng dáº«n MinIO: {minio_path}")
    
    try:
        # Äá»c Parquet tá»« MinIO
        df = spark.read.parquet(minio_path)
        
        # Kiá»ƒm tra dá»¯ liá»‡u
        record_count = df.count()
        print(f"âœ… ÄÃ£ Ä‘á»c {record_count:,} báº£n ghi tá»« MinIO")
        
        if record_count == 0:
            raise ValueError("KhÃ´ng cÃ³ dá»¯ liá»‡u trong MinIO! Vui lÃ²ng cháº¡y ETL pipeline trÆ°á»›c.")
        
        # Hiá»ƒn thá»‹ schema
        print("\nğŸ“Š Schema dá»¯ liá»‡u:")
        df.printSchema()
        
        # Hiá»ƒn thá»‹ sample
        print("\nğŸ“‹ Sample dá»¯ liá»‡u (5 dÃ²ng Ä‘áº§u):")
        df.select("datetime", "city", "temperature", "humidity", "pressure", "wind_speed").show(5, truncate=False)
        
        return df
        
    except Exception as e:
        print(f"âŒ Lá»—i Ä‘á»c dá»¯ liá»‡u tá»« MinIO: {e}")
        print(f"ğŸ’¡ Kiá»ƒm tra:")
        print(f"   1. MinIO server Ä‘ang cháº¡y")
        print(f"   2. ÄÆ°á»ng dáº«n: {minio_path}")
        print(f"   3. Bucket vÃ  folder tá»“n táº¡i")
        raise


# ===========================
# CHUáº¨N Bá»Š Dá»® LIá»†U CHO TIME SERIES
# ===========================

def prepare_time_series_data(df, city=None):
    """
    Chuáº©n bá»‹ dá»¯ liá»‡u time series cho má»™t thÃ nh phá»‘
    
    Args:
        df: Spark DataFrame
        city: TÃªn thÃ nh phá»‘ (None náº¿u muá»‘n xá»­ lÃ½ táº¥t cáº£)
    
    Returns:
        pandas.DataFrame: Dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c sáº¯p xáº¿p theo thá»i gian
    """
    print(f"\nâš™ï¸  Chuáº©n bá»‹ dá»¯ liá»‡u time series cho {city if city else 'táº¥t cáº£ cÃ¡c thÃ nh phá»‘'}...")
    
    # Lá»c theo thÃ nh phá»‘ náº¿u cÃ³
    if city:
        df_city = df.filter(col("city") == city)
    else:
        df_city = df
    
    # Chá»n cÃ¡c cá»™t cáº§n thiáº¿t vÃ  sáº¯p xáº¿p theo thá»i gian
    df_prepared = df_city.select(
        "datetime",
        "city",
        "temperature",
        "humidity", 
        "pressure",
        "wind_speed",
        "wind_direction"
    ).filter(
        col("temperature").isNotNull() &
        col("humidity").isNotNull() &
        col("pressure").isNotNull() &
        col("wind_speed").isNotNull()
    ).orderBy("datetime")
    
    # Chuyá»ƒn sang Pandas Ä‘á»ƒ xá»­ lÃ½ LSTM
    pandas_df = df_prepared.toPandas()
    
    if len(pandas_df) == 0:
        raise ValueError(f"KhÃ´ng cÃ³ dá»¯ liá»‡u cho thÃ nh phá»‘ {city}")
    
    # Äáº£m báº£o datetime lÃ  datetime type
    pandas_df['datetime'] = pd.to_datetime(pandas_df['datetime'])
    
    # Sáº¯p xáº¿p láº¡i theo thá»i gian
    pandas_df = pandas_df.sort_values('datetime').reset_index(drop=True)
    
    # Loáº¡i bá» duplicates theo datetime
    pandas_df = pandas_df.drop_duplicates(subset=['datetime'], keep='last')
    
    print(f"âœ… ÄÃ£ chuáº©n bá»‹ {len(pandas_df)} báº£n ghi")
    print(f"   Thá»i gian: {pandas_df['datetime'].min()} Ä‘áº¿n {pandas_df['datetime'].max()}")
    
    return pandas_df


# ===========================
# XÃ‚Y Dá»°NG VÃ€ HUáº¤N LUYá»†N LSTM
# ===========================

def create_sequences(data, lookback=LOOKBACK_DAYS):
    """
    Táº¡o sequences cho LSTM
    
    Args:
        data: numpy array vá»›i shape (n_samples, n_features)
        lookback: Sá»‘ timesteps trong quÃ¡ khá»© Ä‘á»ƒ dá»± Ä‘oÃ¡n
    
    Returns:
        X, y: Input sequences vÃ  target values
    """
    X, y = [], []
    for i in range(lookback, len(data)):
        X.append(data[i-lookback:i])
        y.append(data[i])
    return np.array(X), np.array(y)


def build_lstm_model(input_shape, n_features):
    """
    XÃ¢y dá»±ng mÃ´ hÃ¬nh LSTM
    
    Args:
        input_shape: Shape cá»§a input (lookback, n_features)
        n_features: Sá»‘ lÆ°á»£ng features
    
    Returns:
        keras.Model: MÃ´ hÃ¬nh LSTM
    """
    model = Sequential([
        LSTM(50, return_sequences=True, input_shape=input_shape),
        Dropout(0.2),
        LSTM(50, return_sequences=True),
        Dropout(0.2),
        LSTM(50),
        Dropout(0.2),
        Dense(n_features)  # Dá»± Ä‘oÃ¡n táº¥t cáº£ features cÃ¹ng lÃºc
    ])
    
    model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss='mse',
        metrics=['mae']
    )
    
    return model


def train_lstm_model(pandas_df, city_name):
    """
    Huáº¥n luyá»‡n mÃ´ hÃ¬nh LSTM cho má»™t thÃ nh phá»‘
    
    Args:
        pandas_df: DataFrame chá»©a dá»¯ liá»‡u time series
        city_name: TÃªn thÃ nh phá»‘
    
    Returns:
        model: Trained LSTM model
        scaler: MinMaxScaler Ä‘Ã£ fit
    """
    print(f"\nğŸ§  Huáº¥n luyá»‡n LSTM cho {city_name}...")
    
    if not TF_AVAILABLE:
        raise ImportError("TensorFlow khÃ´ng Ä‘Æ°á»£c cÃ i Ä‘áº·t!")
    
    # Chá»n features
    feature_data = pandas_df[FEATURE_COLUMNS].values
    
    # Chuáº©n hÃ³a dá»¯ liá»‡u
    scaler = MinMaxScaler()
    scaled_data = scaler.fit_transform(feature_data)
    
    # Táº¡o sequences
    lookback = LOOKBACK_DAYS * 24  # Giáº£ sá»­ dá»¯ liá»‡u theo giá» (24 giá»/ngÃ y)
    if len(scaled_data) < lookback + FORECAST_DAYS * 24:
        lookback = max(7 * 24, len(scaled_data) // 4)  # Äiá»u chá»‰nh náº¿u khÃ´ng Ä‘á»§ dá»¯ liá»‡u
        print(f"   âš ï¸  Äiá»u chá»‰nh lookback thÃ nh {lookback} timesteps")
    
    X, y = create_sequences(scaled_data, lookback)
    
    if len(X) == 0:
        raise ValueError(f"KhÃ´ng Ä‘á»§ dá»¯ liá»‡u Ä‘á»ƒ táº¡o sequences. Cáº§n Ã­t nháº¥t {lookback + 1} timesteps")
    
    # Chia train/test
    train_size = int(len(X) * 0.8)
    X_train, X_test = X[:train_size], X[train_size:]
    y_train, y_test = y[:train_size], y[train_size:]
    
    print(f"   ğŸ“Š Training samples: {len(X_train)}")
    print(f"   ğŸ“Š Test samples: {len(X_test)}")
    
    # XÃ¢y dá»±ng mÃ´ hÃ¬nh
    input_shape = (X_train.shape[1], X_train.shape[2])
    model = build_lstm_model(input_shape, len(FEATURE_COLUMNS))
    
    print(f"   ğŸ—ï¸  MÃ´ hÃ¬nh LSTM:")
    model.summary()
    
    # Huáº¥n luyá»‡n
    print(f"   ğŸš€ Báº¯t Ä‘áº§u huáº¥n luyá»‡n ({EPOCHS} epochs)...")
    history = model.fit(
        X_train, y_train,
        batch_size=BATCH_SIZE,
        epochs=EPOCHS,
        validation_data=(X_test, y_test),
        verbose=1
    )
    
    # ÄÃ¡nh giÃ¡
    train_loss = model.evaluate(X_train, y_train, verbose=0)
    test_loss = model.evaluate(X_test, y_test, verbose=0)
    
    print(f"\n   âœ… Huáº¥n luyá»‡n hoÃ n táº¥t!")
    print(f"   ğŸ“ˆ Train Loss: {train_loss[0]:.4f}, Train MAE: {train_loss[1]:.4f}")
    print(f"   ğŸ“ˆ Test Loss: {test_loss[0]:.4f}, Test MAE: {test_loss[1]:.4f}")
    
    return model, scaler, lookback


# ===========================
# Dá»° ÄOÃN 7 NGÃ€Y Tá»šI
# ===========================

def forecast_next_7_days(model, scaler, pandas_df, lookback, city_name):
    """
    Dá»± Ä‘oÃ¡n thá»i tiáº¿t 7 ngÃ y tá»›i
    
    Args:
        model: Trained LSTM model
        scaler: MinMaxScaler
        pandas_df: Historical data
        lookback: Lookback window size
        city_name: TÃªn thÃ nh phá»‘
    
    Returns:
        pandas.DataFrame: DataFrame chá»©a dá»± Ä‘oÃ¡n
    """
    print(f"\nğŸ”® Dá»± Ä‘oÃ¡n thá»i tiáº¿t 7 ngÃ y tá»›i cho {city_name}...")
    
    # Láº¥y dá»¯ liá»‡u cuá»‘i cÃ¹ng Ä‘á»ƒ lÃ m input
    feature_data = pandas_df[FEATURE_COLUMNS].values
    scaled_data = scaler.transform(feature_data)
    
    # Láº¥y sequence cuá»‘i cÃ¹ng
    last_sequence = scaled_data[-lookback:].reshape(1, lookback, len(FEATURE_COLUMNS))
    
    # Dá»± Ä‘oÃ¡n tá»«ng bÆ°á»›c (hourly)
    predictions = []
    current_input = last_sequence.copy()
    
    # Dá»± Ä‘oÃ¡n 7 ngÃ y = 7 * 24 = 168 giá»
    forecast_hours = FORECAST_DAYS * 24
    
    for i in range(forecast_hours):
        # Dá»± Ä‘oÃ¡n timestep tiáº¿p theo
        next_pred = model.predict(current_input, verbose=0)
        predictions.append(next_pred[0])
        
        # Cáº­p nháº­t input cho láº§n dá»± Ä‘oÃ¡n tiáº¿p theo
        # ThÃªm prediction vÃ o cuá»‘i vÃ  bá» Ä‘áº§u
        current_input = np.append(current_input[:, 1:, :], next_pred.reshape(1, 1, -1), axis=1)
    
    # Chuyá»ƒn Ä‘á»•i predictions vá» scale gá»‘c
    predictions_array = np.array(predictions)
    predictions_original = scaler.inverse_transform(predictions_array)
    
    # Táº¡o DataFrame vá»›i dá»± Ä‘oÃ¡n
    last_datetime = pd.to_datetime(pandas_df['datetime'].max())
    forecast_dates = pd.date_range(
        start=last_datetime + timedelta(hours=1),
        periods=forecast_hours,
        freq='H'
    )
    
    # Xá»­ lÃ½ wind_direction náº¿u khÃ´ng cÃ³ trong features
    wind_dir_values = predictions_original[:, 4] if len(FEATURE_COLUMNS) > 4 else np.zeros(len(predictions_original))
    
    forecast_df = pd.DataFrame({
        'city': city_name,
        'forecast_datetime': forecast_dates,
        'forecast_date': forecast_dates.date,  # Sáº½ Ä‘Æ°á»£c convert sau
        'temperature': predictions_original[:, 0],
        'humidity': predictions_original[:, 1],
        'pressure': predictions_original[:, 2],
        'wind_speed': predictions_original[:, 3],
        'wind_direction': wind_dir_values,
        'model_version': 'LSTM_v1.0',
        'prediction_timestamp': pd.Timestamp.now(),
        'confidence_score': 0.85  # CÃ³ thá»ƒ tÃ­nh toÃ¡n dá»±a trÃªn validation loss
    })
    
    # Äáº£m báº£o forecast_date lÃ  date type (khÃ´ng pháº£i datetime)
    forecast_df['forecast_date'] = pd.to_datetime(forecast_df['forecast_datetime']).dt.date
    
    print(f"âœ… ÄÃ£ táº¡o {len(forecast_df)} dá»± Ä‘oÃ¡n")
    print(f"   Tá»« {forecast_dates[0]} Ä‘áº¿n {forecast_dates[-1]}")
    
    return forecast_df


# ===========================
# GHI VÃ€O POSTGRESQL
# ===========================

def write_forecasts_to_postgres(spark, forecast_df):
    """
    Ghi káº¿t quáº£ dá»± Ä‘oÃ¡n vÃ o PostgreSQL
    
    Args:
        spark: SparkSession
        forecast_df: pandas.DataFrame chá»©a dá»± Ä‘oÃ¡n
    """
    print("\n" + "="*80)
    print("ğŸ’¾ GHI Káº¾T QUáº¢ VÃ€O POSTGRESQL")
    print("="*80)
    
    # Chuyá»ƒn pandas DataFrame sang Spark DataFrame
    spark_df = spark.createDataFrame(forecast_df)
    
    # Äá»•i tÃªn cá»™t Ä‘á»ƒ khá»›p vá»›i schema PostgreSQL
    spark_df = spark_df.select(
        col("city").alias("city"),
        col("forecast_date").alias("forecast_date"),
        col("forecast_datetime").alias("forecast_datetime"),
        col("temperature").alias("temperature_celsius"),
        col("humidity").alias("humidity_pct"),
        col("pressure").alias("pressure_hpa"),
        col("wind_speed").alias("wind_speed_kmh"),
        col("wind_direction").alias("wind_direction_deg"),
        col("model_version").alias("model_version"),
        col("prediction_timestamp").alias("prediction_timestamp"),
        col("confidence_score").alias("confidence_score")
    )
    
    print(f"ğŸ“Š Sá»‘ lÆ°á»£ng dá»± Ä‘oÃ¡n: {spark_df.count()}")
    print("\nğŸ“‹ Sample dá»¯ liá»‡u sáº½ ghi:")
    spark_df.show(10, truncate=False)
    
    # Ghi vÃ o PostgreSQL
    try:
        jdbc_config = postgres_config.get_spark_jdbc_config()
        jdbc_props = postgres_config.get_spark_jdbc_properties()
        
        print(f"\nğŸ”— Káº¿t ná»‘i PostgreSQL:")
        print(f"   URL: {jdbc_config['url']}")
        print(f"   Table: {postgres_config.FORECAST_TABLE}")
        
        # Ghi vá»›i mode append (cÃ³ thá»ƒ cÃ³ duplicates, sáº½ xá»­ lÃ½ báº±ng UNIQUE constraint)
        spark_df.write \
            .format("jdbc") \
            .option("url", jdbc_config['url']) \
            .option("dbtable", postgres_config.FORECAST_TABLE) \
            .option("user", jdbc_props['user']) \
            .option("password", jdbc_props['password']) \
            .option("driver", jdbc_props['driver']) \
            .mode("append") \
            .save()
        
        print("âœ… ÄÃ£ ghi thÃ nh cÃ´ng vÃ o PostgreSQL!")
        
    except Exception as e:
        print(f"âŒ Lá»—i ghi vÃ o PostgreSQL: {e}")
        print(f"ğŸ’¡ Kiá»ƒm tra:")
        print(f"   1. PostgreSQL server Ä‘ang cháº¡y")
        print(f"   2. Database vÃ  table Ä‘Ã£ Ä‘Æ°á»£c táº¡o")
        print(f"   3. Credentials Ä‘Ãºng")
        raise


# ===========================
# MAIN PIPELINE
# ===========================

def main():
    """HÃ m main Ä‘á»ƒ cháº¡y toÃ n bá»™ pipeline"""
    print("\n" + "="*80)
    print("ğŸš€ SPARK ML - LSTM FORECAST PIPELINE")
    print("="*80)
    print(f"ğŸ“… Dá»± Ä‘oÃ¡n {FORECAST_DAYS} ngÃ y tá»›i")
    print(f"ğŸ“Š Lookback: {LOOKBACK_DAYS} ngÃ y")
    print("="*80)
    
    if not TF_AVAILABLE:
        print("\nâŒ TensorFlow khÃ´ng Ä‘Æ°á»£c cÃ i Ä‘áº·t!")
        print("ğŸ’¡ CÃ i Ä‘áº·t: pip install tensorflow")
        sys.exit(1)
    
    # Khá»Ÿi táº¡o Spark
    spark = create_spark_session()
    
    try:
        # 1. Äá»c dá»¯ liá»‡u tá»« MinIO
        df = read_data_from_minio(spark)
        
        # 2. Láº¥y danh sÃ¡ch cÃ¡c thÃ nh phá»‘
        cities = df.select("city").distinct().rdd.map(lambda r: r[0]).collect()
        print(f"\nğŸ™ï¸  TÃ¬m tháº¥y {len(cities)} thÃ nh phá»‘: {cities[:5]}...")
        
        # 3. Xá»­ lÃ½ tá»«ng thÃ nh phá»‘ (hoáº·c chá»‰ thÃ nh phá»‘ Ä‘áº§u tiÃªn Ä‘á»ƒ demo)
        all_forecasts = []
        
        # Chá»‰ xá»­ lÃ½ 3 thÃ nh phá»‘ Ä‘áº§u tiÃªn Ä‘á»ƒ demo (cÃ³ thá»ƒ thay Ä‘á»•i)
        cities_to_process = cities[:3] if len(cities) > 3 else cities
        
        for city in cities_to_process:
            try:
                print(f"\n{'='*80}")
                print(f"ğŸ™ï¸  Xá»¬ LÃ THÃ€NH PHá»: {city}")
                print(f"{'='*80}")
                
                # Chuáº©n bá»‹ dá»¯ liá»‡u
                city_df = prepare_time_series_data(df, city)
                
                # Huáº¥n luyá»‡n LSTM
                model, scaler, lookback = train_lstm_model(city_df, city)
                
                # Dá»± Ä‘oÃ¡n
                forecast_df = forecast_next_7_days(model, scaler, city_df, lookback, city)
                
                all_forecasts.append(forecast_df)
                
            except Exception as e:
                print(f"âŒ Lá»—i xá»­ lÃ½ thÃ nh phá»‘ {city}: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        # 4. Gá»™p táº¥t cáº£ dá»± Ä‘oÃ¡n vÃ  ghi vÃ o PostgreSQL
        if all_forecasts:
            combined_forecasts = pd.concat(all_forecasts, ignore_index=True)
            write_forecasts_to_postgres(spark, combined_forecasts)
            
            print("\n" + "="*80)
            print("âœ… HOÃ€N Táº¤T PIPELINE!")
            print("="*80)
            print(f"ğŸ“Š Tá»•ng sá»‘ dá»± Ä‘oÃ¡n: {len(combined_forecasts)}")
            print(f"ğŸ™ï¸  Sá»‘ thÃ nh phá»‘: {len(cities_to_process)}")
            print(f"ğŸ“… Dá»± Ä‘oÃ¡n tá»« {combined_forecasts['forecast_datetime'].min()} Ä‘áº¿n {combined_forecasts['forecast_datetime'].max()}")
        else:
            print("\nâŒ KhÃ´ng cÃ³ dá»± Ä‘oÃ¡n nÃ o Ä‘Æ°á»£c táº¡o!")
        
    except Exception as e:
        print(f"\nâŒ Lá»–I: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    
    finally:
        spark.stop()
        print("\nğŸ‘‹ ÄÃ£ dá»«ng Spark Session")


if __name__ == "__main__":
    main()

