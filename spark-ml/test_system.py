"""
Test Script - Test MinIO Connection and Data Loading
Ki·ªÉm tra k·∫øt n·ªëi MinIO v√† ƒë·ªçc d·ªØ li·ªáu
"""

from pyspark.sql import SparkSession
import config

def test_spark_minio_connection():
    """Test Spark connection to MinIO"""
    print("\n" + "="*80)
    print("üß™ TESTING MINIO CONNECTION")
    print("="*80)
    
    # Create Spark session
    packages = [
        "org.apache.hadoop:hadoop-aws:3.3.4",
        "com.amazonaws:aws-java-sdk-bundle:1.12.262"
    ]
    
    builder = SparkSession.builder \
        .appName("TestMinIO") \
        .master("local[*]") \
        .config("spark.jars.packages", ",".join(packages)) \
        .config("spark.jars.ivy", "/tmp/.ivy2")
    
    # Add S3A config
    for key, value in config.SPARK_S3A_CONFIG.items():
        builder = builder.config(key, value)
    
    spark = builder.getOrCreate()
    spark.sparkContext.setLogLevel("WARN")
    
    print("‚úÖ Spark session created")
    
    try:
        # Try to read from MinIO
        print(f"\nüìÇ Attempting to read from: {config.MINIO_INPUT_PATH}")
        
        df = spark.read.parquet(config.MINIO_INPUT_PATH)
        
        count = df.count()
        print(f"‚úÖ Successfully read {count} records from MinIO!")
        
        print("\nüìä Schema:")
        df.printSchema()
        
        print("\nüìã Sample data:")
        df.show(5, truncate=False)
        
        return True
        
    except Exception as e:
        print(f"\n‚ùå Error reading from MinIO: {e}")
        print("\nPossible causes:")
        print("  1. MinIO is not running")
        print("  2. No data in MinIO (run ETL pipeline first)")
        print("  3. Incorrect MinIO credentials")
        print("  4. Network connection issues")
        return False
        
    finally:
        spark.stop()


def test_modules_import():
    """Test that all modules can be imported"""
    print("\n" + "="*80)
    print("üß™ TESTING MODULE IMPORTS")
    print("="*80)
    
    try:
        print("Importing data_loader...", end=" ")
        from data_loader import WeatherDataLoader
        print("‚úÖ")
        
        print("Importing feature_engineering...", end=" ")
        from feature_engineering import TimeSeriesFeatureEngineer
        print("‚úÖ")
        
        print("Importing models...", end=" ")
        from models import WeatherForecastModels
        print("‚úÖ")
        
        print("Importing forecast_evaluator...", end=" ")
        from forecast_evaluator import ForecastEvaluator
        print("‚úÖ")
        
        print("Importing visualization...", end=" ")
        from visualization import ForecastVisualizer
        print("‚úÖ")
        
        print("\n‚úÖ All modules imported successfully!")
        return True
        
    except Exception as e:
        print(f"\n‚ùå Import error: {e}")
        return False


def main():
    """Run all tests"""
    print("\n" + "="*80)
    print("üöÄ WEATHER FORECASTING ML - SYSTEM TESTS")
    print("="*80)
    
    # Test 1: Module imports
    test1 = test_modules_import()
    
    # Test 2: MinIO connection
    test2 = test_spark_minio_connection()
    
    # Summary
    print("\n" + "="*80)
    print("üìù TEST SUMMARY")
    print("="*80)
    print(f"Module Imports:      {'‚úÖ PASS' if test1 else '‚ùå FAIL'}")
    print(f"MinIO Connection:    {'‚úÖ PASS' if test2 else '‚ùå FAIL'}")
    print("="*80 + "\n")
    
    if test1 and test2:
        print("‚úÖ ALL TESTS PASSED! System is ready to use.")
        print("\nNext steps:")
        print("  python weather_forecasting.py")
    else:
        print("‚ö†Ô∏è  SOME TESTS FAILED. Please check the errors above.")
        if not test2:
            print("\nüí° If MinIO test failed, make sure:")
            print("   1. MinIO is running (docker ps)")
            print("   2. ETL pipeline has written data to MinIO")
            print("   3. Check spark_etl_weather_disaster/main_etl.py")


if __name__ == "__main__":
    main()
