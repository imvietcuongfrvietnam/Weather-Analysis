from pyspark.sql import SparkSession
from pyspark.sql.functions import udf, col, when
from pyspark.sql.types import StringType
from pyspark.ml.feature import VectorAssembler, StringIndexer, StandardScaler
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml import Pipeline
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

# --- Cáº¤U HÃŒNH ---
# âš ï¸ Äáº£m báº£o HDFS Cluster cá»§a báº¡n Ä‘Ã£ cháº¡y vÃ  Spark Ä‘Ã£ Ä‘Æ°á»£c cáº¥u hÃ¬nh Ä‘á»ƒ káº¿t ná»‘i HDFS (HDFS_CONF)
# ÄÆ°á»ng dáº«n dá»¯ liá»‡u máº«u (Giáº£ sá»­ báº¡n Ä‘Ã£ lÆ°u dá»¯ liá»‡u thÃ´ vÃ o HDFS)
#HDFS_INPUT_PATH = "hdfs://namenode:9000/nyc_data/weather/raw_all_years.csv"
# ÄÆ°á»ng dáº«n lÆ°u Model Ä‘Ã£ huáº¥n luyá»‡n
#HDFS_MODEL_OUTPUT = "hdfs://namenode:9000/models/weather_rf_classifier"

# 1. KHá»I Táº O SPARK SESSION
spark = SparkSession.builder \
    .appName("WeatherClassifierTraining") \
    .master("local[*]") \
    .getOrCreate()
    
spark.sparkContext.setLogLevel("WARN")
print("âœ… Spark Session Ä‘Ã£ khá»Ÿi Ä‘á»™ng.")


# --- 2. LOGIC NGHIá»†P Vá»¤ (HÃ m Gom nhÃ³m cá»§a báº¡n) ---
def define_weather_group(desc):
    """Gom nhÃ³m thá»i tiáº¿t (Chuyá»ƒn tá»« Pandas logic sang PySpark UDF)"""
    desc = str(desc).lower()
    
    # NhÃ³m 1: MÆ°a/BÃ£o/Tuyáº¿t (Precipitation)
    precipitation_keywords = ['rain', 'snow', 'thunderstorm', 'drizzle', 'sleet', 'squall', 'shower']
    if any(x in desc for x in precipitation_keywords):
        return 'Precipitation'
    
    # NhÃ³m 2: MÃ¢y/SÆ°Æ¡ng/Bá»¥i (Cloudy/Fog)
    cloudy_keywords = ['cloud', 'fog', 'mist', 'haze', 'smoke', 'dust', 'sand', 'ash']
    if any(x in desc for x in cloudy_keywords):
        return 'Cloudy/Fog'
    
    # NhÃ³m 3: Trá»i quang (Clear)
    elif 'clear' in desc:
        return 'Clear'
    
    else:
        return 'Cloudy/Fog'

# ÄÄƒng kÃ½ hÃ m gom nhÃ³m thÃ nh UDF (User Defined Function)
define_weather_group_udf = udf(define_weather_group, StringType())


# --- 3. Äá»ŒC VÃ€ CHUáº¨N Bá»Š Dá»® LIá»†U (Giáº£ Ä‘á»‹nh Ä‘á»c tá»« CSV Ä‘Ã£ Ä‘áº©y lÃªn HDFS) ---
try:
    df_raw = spark.read.csv(HDFS_INPUT_PATH, header=True, inferSchema=True)
except Exception as e:
    print(f"âŒ Lá»—i Ä‘á»c HDFS: {e}. Vui lÃ²ng kiá»ƒm tra láº¡i {HDFS_INPUT_PATH}")
    # Náº¿u lá»—i, táº¡o DF giáº£ Ä‘á»ƒ code cháº¡y tiáº¿p
    data = [
        (15.0, 60.0, 1010.0, 180.0, 5.0, 'light rain'),
        (25.5, 75.0, 1015.0, 90.0, 1.2, 'sky is clear'),
        (10.1, 95.0, 1005.0, 270.0, 0.5, 'fog')
    ]
    columns = ['temperature', 'humidity', 'pressure', 'wind_direction', 'wind_speed', 'weather_desc']
    df_raw = spark.createDataFrame(data, columns)


# 4. CHUYá»‚N Äá»”I (TRANSFORMATION)
df = df_raw.withColumn('weather_group', define_weather_group_udf(col('weather_desc')))

# Loáº¡i bá» cÃ¡c hÃ ng cÃ³ giÃ¡ trá»‹ null
df = df.dropna(subset=['temperature', 'humidity', 'pressure', 'wind_direction', 'wind_speed', 'weather_group'])

# --- BÆ¯á»šC 5: XÃ‚Y Dá»°NG PIPELINE ML ---

# A. Chuyá»ƒn Ä‘á»•i nhÃ£n chuá»—i thÃ nh sá»‘ (StringIndexer)
# ÄÃ¢y lÃ  bÆ°á»›c báº¯t buá»™c cho cÃ¡c thuáº­t toÃ¡n ML
indexer = StringIndexer(inputCol="weather_group", outputCol="label")

# B. Gom cÃ¡c Features láº¡i thÃ nh 1 Vector (VectorAssembler)
features = ['temperature', 'humidity', 'pressure', 'wind_direction', 'wind_speed']
assembler = VectorAssembler(inputCols=features, outputCol="features_unscaled")

# C. Chuáº©n hÃ³a dá»¯ liá»‡u (StandardScaler)
# Quan trá»ng cho cÃ¡c mÃ´ hÃ¬nh dá»±a trÃªn khoáº£ng cÃ¡ch
scaler = StandardScaler(inputCol="features_unscaled", outputCol="features",
                        withStd=True, withMean=False)

# D. MÃ´ hÃ¬nh PhÃ¢n loáº¡i (RandomForestClassifier)
# Thay max_depth=20 vÃ  n_estimators=100 nhÆ° trong code sklearn cá»§a báº¡n
rf = RandomForestClassifier(labelCol="label", featuresCol="features",
                            numTrees=100, maxDepth=20, 
                            seed=42)

# E. Äá»‹nh nghÄ©a cÃ¡c bÆ°á»›c trong Pipeline
pipeline = Pipeline(stages=[assembler, scaler, indexer, rf])


# --- 6. HUáº¤N LUYá»†N VÃ€ ÄÃNH GIÃ ---

# Chia Train/Test (Spark khÃ´ng dÃ¹ng stratify Ä‘Æ¡n giáº£n nhÆ° sklearn, dÃ¹ng randomSplit)
(trainingData, testData) = df.randomSplit([0.85, 0.15], seed=42)
print("ğŸš€ Báº¯t Ä‘áº§u huáº¥n luyá»‡n mÃ´ hÃ¬nh Random Forest trÃªn Spark Cluster...")

# Huáº¥n luyá»‡n mÃ´ hÃ¬nh
model = pipeline.fit(trainingData)
print("âœ… Huáº¥n luyá»‡n hoÃ n táº¥t!")

# Dá»± Ä‘oÃ¡n trÃªn táº­p Test
predictions = model.transform(testData)

# ÄÃ¡nh giÃ¡ (Sá»­ dá»¥ng F1-Score lÃ  chuáº©n)
evaluator = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="f1")
f1_score = evaluator.evaluate(predictions)
accuracy = evaluator.evaluate(predictions, {evaluator.metricName: "accuracy"})

print("\n--- Káº¾T QUáº¢ ÄÃNH GIÃ TRÃŠN SPARK ---")
print(f"Äá»™ chÃ­nh xÃ¡c (Accuracy): {accuracy:.4f}")
print(f"F1 Score (Äá»™ cÃ¢n báº±ng): {f1_score:.4f}")


# --- 7. LÆ¯U CHECKPOINT MÃ” HÃŒNH VÃ€O HDFS ---
try:
    print(f"\nğŸ’¾ Äang lÆ°u mÃ´ hÃ¬nh vÃ o HDFS táº¡i: {HDFS_MODEL_OUTPUT}")
    # XÃ³a thÆ° má»¥c cÅ© náº¿u tá»“n táº¡i
    spark._jsc.hadoopConfiguration().set("fs.hdfs.impl", "org.apache.hadoop.hdfs.DistributedFileSystem")
    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())
    if fs.exists(spark._jvm.org.apache.hadoop.fs.Path(HDFS_MODEL_OUTPUT)):
        fs.delete(spark._jvm.org.apache.hadoop.fs.Path(HDFS_MODEL_OUTPUT), True)
        print("   -> ÄÃ£ xÃ³a phiÃªn báº£n cÅ©.")
        
    # LÆ°u mÃ´ hÃ¬nh (Tá»± Ä‘á»™ng lÆ°u táº¥t cáº£ cÃ¡c bÆ°á»›c cá»§a Pipeline)
    model.write().overwrite().save(HDFS_MODEL_OUTPUT)
    print("âœ… LÆ°u mÃ´ hÃ¬nh thÃ nh cÃ´ng! MÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c checkpoint.")
    
except Exception as e:
    print(f"âŒ Lá»—i lÆ°u mÃ´ hÃ¬nh vÃ o HDFS: {e}")


spark.stop()