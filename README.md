# H·ªá th·ªëng Ph√¢n t√≠ch & D·ª± b√°o Th·ªùi ti·∫øt Big Data (Weather Analysis System)

ƒê√¢y l√† ƒë·ªì √°n x√¢y d·ª±ng h·ªá th·ªëng Big Data x·ª≠ l√Ω d·ªØ li·ªáu th·ªùi ti·∫øt theo ki·∫øn tr√∫c **Lambda Architecture**, k·∫øt h·ª£p x·ª≠ l√Ω lu·ªìng th·ªùi gian th·ª±c (Real-time Streaming) v√† x·ª≠ l√Ω l√¥ (Batch Processing) ƒë·ªÉ ƒë∆∞a ra d·ª± b√°o ch√≠nh x√°c.

H·ªá th·ªëng ƒë∆∞·ª£c tri·ªÉn khai ho√†n to√†n tr√™n n·ªÅn t·∫£ng **Kubernetes**.

---
## Architecture:
![Architecture](images/pipeline.png)
## Th√†nh vi√™n th·ª±c hi·ªán (Team Members)

* **Nguy·ªÖn Vi·ªát C∆∞·ªùng** - 20224831
* **Tr·ªãnh Vi·ªát C∆∞·ªùng** - 20224941
* **Nguy·ªÖn Danh Gia Minh** - 20224879
* **Ho√†ng Anh Quy·ªÅn** - 20224893
* **L√™ Minh T√∫** - 20215657

---

## üèóÔ∏è Ki·∫øn tr√∫c H·ªá th·ªëng (System Architecture)

H·ªá th·ªëng tu√¢n th·ªß m√¥ h√¨nh **Lambda Architecture** v·ªõi 3 l·ªõp ch√≠nh:

1.  **Speed Layer (L·ªõp T·ªëc ƒë·ªô):**
    * D·ªØ li·ªáu t·ª´ Kafka -> Spark Structured Streaming -> Redis -> Dashboard.
    * M·ª•c ti√™u: Hi·ªÉn th·ªã ch·ªâ s·ªë th·ªùi ti·∫øt hi·ªán t·∫°i v·ªõi ƒë·ªô tr·ªÖ th·∫•p nh·∫•t.

2.  **Batch Layer (L·ªõp X·ª≠ l√Ω L√¥):**
    * D·ªØ li·ªáu t·ª´ Kafka -> Spark Streaming -> MinIO (Data Lake) -> Spark ML (Airflow Trigger) -> PostgreSQL.
    * M·ª•c ti√™u: L∆∞u tr·ªØ d√†i h·∫°n v√† hu·∫•n luy·ªán m√¥ h√¨nh Machine Learning ƒë·ªÉ d·ª± b√°o t∆∞∆°ng lai.

3.  **Serving Layer (L·ªõp Ph·ª•c v·ª•):**
    * Streamlit App: T·ªïng h·ª£p d·ªØ li·ªáu t·ª´ Redis (Real-time) v√† PostgreSQL (Forecast) ƒë·ªÉ hi·ªÉn th·ªã bi·ªÉu ƒë·ªì tr·ª±c quan.

## üõ†Ô∏è C√¥ng ngh·ªá s·ª≠ d·ª•ng (Tech Stack)

* **Ingestion:** Apache Kafka (Strimzi/Bitnami).
* **Processing:** Apache Spark (PySpark), Spark MLlib.
* **Orchestration:** Apache Airflow.
* **Storage:** MinIO (S3 Compatible), PostgreSQL, Redis.
* **Visualization:** Streamlit.
* **Infrastructure:** Docker, Kubernetes (Minikube).

---

## üìÇ C·∫•u tr√∫c d·ª± √°n

.
‚îú‚îÄ‚îÄ airflow/                # M√£ ngu·ªìn & DAGs cho Apache Airflow
‚îú‚îÄ‚îÄ app/                    # ·ª®ng d·ª•ng Dashboard (Streamlit)
‚îú‚îÄ‚îÄ connectors/             # C√°c module k·∫øt n·ªëi database (Redis, Postgres)
‚îú‚îÄ‚îÄ deploy/                 # File c·∫•u h√¨nh Kubernetes (YAML)
‚îú‚îÄ‚îÄ job/                    # M√£ ngu·ªìn Spark (Streaming & Batch ML)
‚îú‚îÄ‚îÄ spark/                  # Config v√† logic l√µi c·ªßa Spark
‚îú‚îÄ‚îÄ weather_producer.py     # Script gi·∫£ l·∫≠p d·ªØ li·ªáu g·ª≠i v√†o Kafka
‚îú‚îÄ‚îÄ requirements.txt        # C√°c th∆∞ vi·ªán Python c·∫ßn thi·∫øt
‚îî‚îÄ‚îÄ README.md               # File t√†i li·ªáu n√†y

---

## üöÄ H∆∞·ªõng d·∫´n V·∫≠n h√†nh (Quick Start)

### 1. Kh·ªüi ƒë·ªông H·∫° t·∫ßng
Y√™u c·∫ßu m√°y ƒë√£ c√†i Minikube v√† kubectl.

cd deploy
kubectl apply -f .

### 2. ƒê·∫©y Code l√™n Airflow
S·ª≠ d·ª•ng script ƒë·ªÉ ƒë·ªìng b·ªô code t·ª´ m√°y local l√™n Pod Scheduler.

SCHEDULER_POD=#(kubectl get pods -n airflow -l component=scheduler -o jsonpath='{.items[0].metadata.name}')
kubectl cp airflow/. airflow/#SCHEDULER_POD:/opt/airflow/dags/
kubectl cp job/ airflow/#SCHEDULER_POD:/opt/airflow/dags/job/

### 3. K√≠ch ho·∫°t lu·ªìng d·ªØ li·ªáu (Data Generator)
Ch·∫°y script Producer ·ªü m√°y local ƒë·ªÉ gi·∫£ l·∫≠p d·ªØ li·ªáu th·ªùi ti·∫øt g·ª≠i l√™n K8s.

python weather_producer.py

### 4. Truy c·∫≠p Dashboard
* **Streamlit:** `localhost:8501`
* **Airflow:** `localhost:8080`
* **MinIO:** `localhost:9001`

---

## üéì B√†i h·ªçc kinh nghi·ªám (Lessons Learned)

D∆∞·ªõi ƒë√¢y l√† c√°c th√°ch th·ª©c k·ªπ thu·∫≠t nh√≥m ƒë√£ g·∫∑p ph·∫£i v√† gi·∫£i ph√°p kh·∫Øc ph·ª•c trong qu√° tr√¨nh th·ª±c hi·ªán ƒë·ªì √°n.

### Lesson 1: Data Ingestion & Simulation (M√¥ ph·ªèng d·ªØ li·ªáu)

* **V·∫•n ƒë·ªÅ:** D·ªØ li·ªáu g·ªëc l√† file CSV tƒ©nh, kh√¥ng ph√π h·ª£p ƒë·ªÉ demo Streaming th·ªùi gian th·ª±c.
* **Gi·∫£i ph√°p:** X√¢y d·ª±ng `weather_producer.py` ho·∫°t ƒë·ªông nh∆∞ m·ªôt Kafka Producer, ƒë·ªçc tu·∫ßn t·ª± t·ª´ng d√≤ng CSV, c·∫≠p nh·∫≠t timestamp hi·ªán t·∫°i v√† g·ª≠i li√™n t·ª•c v√†o h·ªá th·ªëng.
* **K·∫øt qu·∫£:** T·∫°o ƒë∆∞·ª£c d√≤ng d·ªØ li·ªáu v√¥ h·∫°n, ·ªïn ƒë·ªãnh ƒë·ªÉ ki·ªÉm th·ª≠ t√≠nh nƒÉng Real-time Dashboard.

### Lesson 2: Handling Cold Start (X·ª≠ l√Ω kh·ªüi ƒë·ªông l·∫°nh)

* **V·∫•n ƒë·ªÅ:** Khi h·ªá th·ªëng m·ªõi ch·∫°y (Cold Start), c√°c t√°c v·ª• Feature Engineering (nh∆∞ t·∫°o Lag features 1h, 3h) t·∫°o ra nhi·ªÅu gi√° tr·ªã NULL. Vi·ªác d√πng `dropna()` khi·∫øn t·∫≠p train b·ªã r·ªóng -> Spark Job crash.
* **Gi·∫£i ph√°p:** Thay ƒë·ªïi chi·∫øn l∆∞·ª£c x·ª≠ l√Ω d·ªØ li·ªáu thi·∫øu. Thay v√¨ x√≥a b·ªè, nh√≥m s·ª≠ d·ª•ng k·ªπ thu·∫≠t `Imputation` (ƒëi·ªÅn gi√° tr·ªã 0 ho·∫∑c trung b√¨nh) cho giai ƒëo·∫°n kh·ªüi ch·∫°y ƒë·∫ßu ti√™n.
* **K·∫øt qu·∫£:** Pipeline ML ho·∫°t ƒë·ªông tr∆°n tru ngay c·∫£ khi m·ªõi thu th·∫≠p d·ªØ li·ªáu ƒë∆∞·ª£c 5 ph√∫t.

### Lesson 3: Spark Schema & Parquet Integration

* **V·∫•n ƒë·ªÅ:** Batch Job ch·∫°y tr∆∞·ªõc khi Streaming Job k·ªãp ghi file d·ªØ li·ªáu, d·∫´n ƒë·∫øn l·ªói `AnalysisException` do Spark kh√¥ng t√¨m th·∫•y metadata c·ªßa Parquet.
* **Gi·∫£i ph√°p:** Thi·∫øt l·∫≠p Airflow Sensor v√† logic ki·ªÉm tra (Try-Catch) ƒë·ªÉ ƒë·∫£m b·∫£o th∆∞ m·ª•c d·ªØ li·ªáu t·ªìn t·∫°i tr∆∞·ªõc khi Spark ML kh·ªüi ch·∫°y.

### Lesson 4: System Integration in Kubernetes (K·∫øt n·ªëi d·ªãch v·ª•)

* **V·∫•n ƒë·ªÅ:** C√°c service (Spark, Redis, Postgres) kh√¥ng th·ªÉ giao ti·∫øp v·ªõi nhau khi d√πng `localhost` b√™n trong Pod.
* **Gi·∫£i ph√°p:** Chu·∫©n h√≥a to√†n b·ªô c·∫•u h√¨nh trong `config.py`, s·ª≠ d·ª•ng **Kubernetes Service Name** (VD: `weather-redis`, `weather-minio`) l√†m ƒë·ªãa ch·ªâ k·∫øt n·ªëi n·ªôi b·ªô.
* **K·∫øt qu·∫£:** H·ªá th·ªëng giao ti·∫øp ·ªïn ƒë·ªãnh, kh√¥ng ph·ª• thu·ªôc v√†o IP ƒë·ªông c·ªßa Pod.

### Lesson 5: Airflow Deployment (Tri·ªÉn khai li√™n t·ª•c)

* **V·∫•n ƒë·ªÅ:** Code s·ª≠a xong nh∆∞ng Airflow kh√¥ng c·∫≠p nh·∫≠t, quy tr√¨nh build l·∫°i Docker Image t·ªën qu√° nhi·ªÅu th·ªùi gian.
* **Gi·∫£i ph√°p:** √Åp d·ª•ng c∆° ch·∫ø **Hot-swapping** b·∫±ng l·ªánh `kubectl cp` k·∫øt h·ª£p v·ªõi restart Pod Scheduler, gi√∫p gi·∫£m th·ªùi gian deploy code t·ª´ 10 ph√∫t xu·ªëng c√≤n 30 gi√¢y.

---
