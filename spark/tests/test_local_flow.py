import sys
import os
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, DoubleType
from minio import Minio # C·∫ßn pip install minio

# --- SETUP ƒê∆Ø·ªúNG D·∫™N ---
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# --- C·∫§U H√åNH C·ªîNG M·ªöI (ƒê·ªÇ N√â C·ªîNG C≈® B·ªä L·ªñI) ---
# Map v·ªõi l·ªánh port-forward b·∫°n v·ª´a ch·∫°y
os.environ["REDIS_HOST"] = "localhost"
os.environ["REDIS_PORT"] = "6380"  # C·ªïng m·ªõi

os.environ["MINIO_ENDPOINT"] = "localhost:9090" # C·ªïng m·ªõi
os.environ["MINIO_ACCESS_KEY"] = "admin"        # User K8s
os.environ["MINIO_SECRET_KEY"] = "password123"  # Pass K8s

# Import modules
from transformations.cleaning import DataCleaner
from transformations.normalization import DataNormalizer
from writers.redis_data_writer import RedisWriter
from config import minio_config

def auto_create_bucket():
    """H√†m t·ª± ƒë·ªông t·∫°o Bucket, kh√¥ng c·∫ßn v√†o Browser"""
    print("üõ†Ô∏è  ƒêang ki·ªÉm tra MinIO Bucket...")
    try:
        client = Minio(
            os.environ["MINIO_ENDPOINT"],
            access_key=os.environ["MINIO_ACCESS_KEY"],
            secret_key=os.environ["MINIO_SECRET_KEY"],
            secure=False
        )
        bucket_name = minio_config.MINIO_BUCKET
        
        if not client.bucket_exists(bucket_name):
            client.make_bucket(bucket_name)
            print(f"   ‚úÖ ƒê√£ T·ª∞ T·∫†O bucket: '{bucket_name}'")
        else:
            print(f"   ‚úÖ Bucket '{bucket_name}' ƒë√£ t·ªìn t·∫°i.")
            
    except Exception as e:
        print(f"   ‚ùå L·ªói k·∫øt n·ªëi MinIO ƒë·ªÉ t·∫°o bucket: {e}")
        print("   üëâ B·∫°n ƒë√£ ch·∫°y 'kubectl port-forward ... 9090:9000' ch∆∞a?")
        sys.exit(1)

def create_local_spark():
    packages = [
        "org.apache.hadoop:hadoop-aws:3.3.4",
        "com.amazonaws:aws-java-sdk-bundle:1.12.262"
    ]
    return SparkSession.builder \
        .appName("LocalTest") \
        .master("local[1]") \
        .config("spark.jars.packages", ",".join(packages)) \
        .config("spark.hadoop.fs.s3a.endpoint", f"http://{os.environ['MINIO_ENDPOINT']}") \
        .config("spark.hadoop.fs.s3a.access.key", os.environ["MINIO_ACCESS_KEY"]) \
        .config("spark.hadoop.fs.s3a.secret.key", os.environ["MINIO_SECRET_KEY"]) \
        .config("spark.hadoop.fs.s3a.path.style.access", "true") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false") \
        .getOrCreate()

def test_full_flow():
    # 0. T·ª∞ T·∫†O BUCKET TR∆Ø·ªöC
    auto_create_bucket()
    
    spark = create_local_spark()
    spark.sparkContext.setLogLevel("ERROR")
    
    print("\n" + "="*50)
    print("üß™ B·∫ÆT ƒê·∫¶U TEST LU·ªíNG D·ªÆ LI·ªÜU (AUTO BUCKET)")
    print("="*50)

    # 1. MOCK DATA
    data = [
        ("2024-01-01 12:00:00", "Hanoi", 25.567, 60.0, 1012.0, " rain ", 10.0, 5.0),
        ("2024-01-01 12:00:00", "BadCity", 1000.0, 60.0, 1012.0, "cloudy", 10.0, 5.0),
    ]
    schema = StructType([
        StructField("datetime", StringType(), True),
        StructField("city", StringType(), True),
        StructField("temperature", DoubleType(), True),
        StructField("humidity", DoubleType(), True),
        StructField("pressure", DoubleType(), True),
        StructField("weather_desc", StringType(), True),
        StructField("wind_direction", DoubleType(), True),
        StructField("wind_speed", DoubleType(), True)
    ])
    df = spark.createDataFrame(data, schema)
    print("1Ô∏è‚É£  Data Mock OK.")

    # 2. TEST CLEANER
    cleaner = DataCleaner()
    df_clean = cleaner.clean_weather_data(df)
    print(f"2Ô∏è‚É£  Cleaner OK (Rows: {df_clean.count()})")

    # 3. TEST NORMALIZER
    normalizer = DataNormalizer()
    df_final = normalizer.normalize_weather_data(df_clean)
    print(f"3Ô∏è‚É£  Normalizer OK")

    # 4. TEST REDIS
    print("4Ô∏è‚É£  Test Redis Writer (Port 6380)...")
    try:
        writer = RedisWriter()
        writer.write_stream_to_redis(df_final, 1)
        print("   ‚úÖ Ghi Redis th√†nh c√¥ng!")
    except Exception as e:
        print(f"   ‚ùå L·ªói Redis: {e}")

    # 5. TEST MINIO
    print("5Ô∏è‚É£  Test MinIO Writer (Port 9090)...")
    try:
        output_path = f"s3a://{minio_config.MINIO_BUCKET}/test_output/weather"
        df_final.write.mode("overwrite").parquet(output_path)
        print(f"   ‚úÖ Ghi MinIO th√†nh c√¥ng t·∫°i: {output_path}")
    except Exception as e:
        print(f"   ‚ùå L·ªói MinIO: {e}")

    spark.stop()

if __name__ == "__main__":
    test_full_flow()