from pyspark.sql import SparkSession
import sys
import os

# =============================================================================
# üõ† FIX C·ª®NG: √âP BU·ªòC ƒê·ªäA CH·ªà KAFKA (ƒê·∫∑t ngay ƒë·∫ßu file)
# =============================================================================
# D√≤ng n√†y ƒë·∫£m b·∫£o d√π Kubernetes qu√™n truy·ªÅn bi·∫øn, Python v·∫´n t·ª± ƒëi·ªÅn v√†o.
# N√≥ ph·∫£i n·∫±m TR∆Ø·ªöC c√°c l·ªánh import kh√°c.
os.environ['KAFKA_BOOTSTRAP_SERVERS'] = "weather-kafka.default.svc.cluster.local:9092"
print(f"üîí HARDCODED KAFKA ADDRESS: {os.environ['KAFKA_BOOTSTRAP_SERVERS']}")
# =============================================================================

# SETUP M√îI TR∆Ø·ªúNG: ∆Øu ti√™n t√¨m config trong folder job hi·ªán t·∫°i
if '/app/job' not in sys.path:
    sys.path.insert(0, '/app/job')
if '/app' not in sys.path:
    sys.path.append('/app')

os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3'
os.environ['PYSPARK_DRIVER_PYTHON'] = '/usr/bin/python3'

try:
    import kafka_config
    import minio_config
    import data_schemas
    
    # C·∫≠p nh·∫≠t l·∫°i config trong b·ªô nh·ªõ n·∫øu module ƒë√£ l·ª° load None
    if hasattr(kafka_config, 'KAFKA_BOOTSTRAP_SERVERS'):
        kafka_config.KAFKA_BOOTSTRAP_SERVERS = os.environ['KAFKA_BOOTSTRAP_SERVERS']
        
    print("‚úÖ Successfully imported all configs and schemas")
except ImportError as e:
    print(f"‚ùå Failed to import configs: {e}")
    sys.exit(1)

from readers.real_data_reader import DataReader
from transformations.cleaning import DataCleaner
from transformations.normalization import DataNormalizer
from writers.redis_data_writer import RedisWriter 
from writers.minio_writer import MinIOWriter

def create_spark_session():
    # Khai b√°o ƒë·ªß 3 g√≥i th∆∞ vi·ªán quan tr·ªçng
    # L∆ØU √ù: ƒê√£ d√πng b·∫£n 3.4.1 cho kh·ªõp v·ªõi Spark 3.4.1
    packages = [
        "org.apache.hadoop:hadoop-aws:3.3.2",
        "com.amazonaws:aws-java-sdk-bundle:1.11.1026",
        "org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1"
    ]
    
    builder = SparkSession.builder \
        .appName("WeatherLambdaArchitecture") \
        .master("local[*]") \
        .config("spark.jars.packages", ",".join(packages)) \
        .config("spark.jars.ivy", "/tmp/.ivy2") 
        
    for key, value in minio_config.SPARK_S3_CONFIG.items():
        builder = builder.config(key, value)
    
    spark = builder.getOrCreate()
    spark.sparkContext.setLogLevel("WARN")
    return spark

def run_etl_pipeline():
    print("üöÄ SPARK PIPELINE STARTING...")
    
    # In ra l·∫ßn cu·ªëi ƒë·ªÉ ch·∫Øc ch·∫Øn
    kafka_addr = os.getenv('KAFKA_BOOTSTRAP_SERVERS')
    print(f"üì° CONNECTING TO KAFKA AT: {kafka_addr}")
    
    if not kafka_addr:
        raise ValueError("CRITICAL: Kafka Address is still NONE! Check the hardcode logic.")

    spark = create_spark_session()
    
    # DataReader s·∫Ω t·ª± ƒë·ªông l·∫•y KAFKA_BOOTSTRAP_SERVERS t·ª´ os.environ ho·∫∑c kafka_config
    reader = DataReader(spark, source_type="kafka", kafka_mode="streaming")
    cleaner = DataCleaner()
    normalizer = DataNormalizer()
    minio_writer = MinIOWriter()
    redis_writer = RedisWriter()
    
    weather_df = reader.read_weather_data()
    weather_clean = cleaner.clean_weather_data(weather_df)
    weather_final = normalizer.normalize_weather_data(weather_clean)
    
    # Ghi MinIO (Data Lake)
    query_minio = minio_writer.write_stream(weather_final, folder="enriched")
    
    # Ghi Redis (Real-time)
    query_redis = weather_final.writeStream \
        .outputMode("append") \
        .foreachBatch(redis_writer.write_stream_to_redis) \
        .option("checkpointLocation", "/tmp/checkpoints/weather_redis") \
        .trigger(processingTime="5 seconds") \
        .start()

    spark.streams.awaitAnyTermination()

if __name__ == "__main__":
    run_etl_pipeline()