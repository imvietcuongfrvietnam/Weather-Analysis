from pyspark.sql import SparkSession
import sys
import os

# --- 1. SETUP M√îI TR∆Ø·ªúNG TUY·ªÜT ƒê·ªêI ---
# Th√™m /app v√†o ƒë·∫ßu sys.path ƒë·ªÉ Python ∆∞u ti√™n t√¨m c√°c folder module g·ªëc
if '/app' not in sys.path:
    sys.path.insert(0, '/app')

# √âp s·ª≠ d·ª•ng Python3 trong container ƒë·ªÉ tr√°nh l·ªói Exit Code 1
os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3'
os.environ['PYSPARK_DRIVER_PYTHON'] = '/usr/bin/python3'

# --- 2. IMPORT MODULES ---
try:
    # S·ª¨A T·∫†I ƒê√ÇY: Import tr·ª±c ti·∫øp file, kh√¥ng d√πng 'from config import'
    import kafka_config
    import minio_config
    import postgres_config
    import redis_config
    print("‚úÖ Successfully imported config files from job folder")
except ImportError as e:
    print(f"‚ùå Failed to import configs: {e}")
    sys.exit(1)# Import c√°c module ch·ª©c nƒÉng t·ª´ root /app
from readers.real_data_reader import DataReader
from transformations.cleaning import DataCleaner
from transformations.normalization import DataNormalizer
from writers.redis_data_writer import RedisWriter 
from writers.minio_writer import MinIOWriter

def create_spark_session():
    # S·ª≠ d·ª•ng version 3.3.0 ƒë·ªÉ kh·ªõp v·ªõi Spark version trong image v3
    packages = [
        "org.apache.hadoop:hadoop-aws:3.3.2",
        "com.amazonaws:aws-java-sdk-bundle:1.11.1026",
        "org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0"
    ]
    
    builder = SparkSession.builder \
        .appName("WeatherLambdaArchitecture") \
        .master("local[*]") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.driver.memory", "800m") \
        .config("spark.executor.memory", "1g") \
        .config("spark.jars.packages", ",".join(packages)) \
        .config("spark.jars.ivy", "/tmp/.ivy2") # Tr√°nh l·ªói quy·ªÅn ghi cache

    # N·∫°p c·∫•u h√¨nh MinIO
    for key, value in minio_config.SPARK_S3_CONFIG.items():
        builder = builder.config(key, value)
    
    spark = builder.getOrCreate()
    spark.sparkContext.setLogLevel("WARN")
    return spark

def run_etl_pipeline():
    print("\n" + "="*80)
    print("üöÄ SPARK PIPELINE STARTING: KAFKA -> (MINIO + REDIS)")
    print("="*80)
    
    spark = create_spark_session()
    
    # Kh·ªüi t·∫°o c√°c th√†nh ph·∫ßn
    reader = DataReader(spark, source_type="kafka", kafka_mode="streaming")
    cleaner = DataCleaner()
    normalizer = DataNormalizer()
    minio_writer = MinIOWriter()
    redis_writer = RedisWriter()
    
    # Lu·ªìng x·ª≠ l√Ω
    weather_df = reader.read_weather_data()
    weather_clean = cleaner.clean_weather_data(weather_df)
    weather_final = normalizer.normalize_weather_data(weather_clean)
    
    # Ghi d·ªØ li·ªáu ƒëa h∆∞·ªõng (MinIO + Redis)
    query_minio = minio_writer.write_stream(weather_final, folder="enriched")

    query_redis = weather_final.writeStream \
        .outputMode("append") \
        .foreachBatch(redis_writer.write_stream_to_redis) \
        .option("checkpointLocation", "/tmp/checkpoints/weather_redis") \
        .trigger(processingTime="5 seconds") \
        .queryName("WriteToRedis") \
        .start()

    print("\n‚úÖ STREAMING QUERIES STARTED. MONITORING DASHBOARD NOW...")
    spark.streams.awaitAnyTermination()

if __name__ == "__main__":
    run_etl_pipeline()