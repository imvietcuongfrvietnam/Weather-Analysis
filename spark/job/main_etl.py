"""
MAIN ETL PIPELINE - KUBERNETES READY
"""

from pyspark.sql import SparkSession
import sys
import os

# --- QUAN TRá»ŒNG: Cáº¥u hÃ¬nh Ä‘Æ°á»ng dáº«n Python trong Container ---
# Trong image Bitnami/Spark, python náº±m á»Ÿ /opt/bitnami/python/bin/python
# NhÆ°ng dÃ¹ng sys.executable lÃ  an toÃ n nháº¥t
os.environ['PYSPARK_PYTHON'] = sys.executable
os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable

# ThÃªm Ä‘Æ°á»ng dáº«n hiá»‡n táº¡i vÃ o path Ä‘á»ƒ import Ä‘Æ°á»£c cÃ¡c module con
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# Import configs
import kafka_config
import minio_config

# Import modules xá»­ lÃ½
from readers.real_data_reader import DataReader
from transformations.cleaning import DataCleaner
from transformations.normalization import DataNormalizer
from transformations.enrichment import DataEnricher
from writers.redis_data_writer import RedisWriter 

def create_spark_session():
    """Khá»Ÿi táº¡o Spark Session tá»‘i Æ°u cho Kubernetes"""
    
    # CÃ¡c thÆ° viá»‡n cáº§n thiáº¿t Ä‘á»ƒ Spark nÃ³i chuyá»‡n vá»›i MinIO (S3)
    packages = [
        "org.apache.hadoop:hadoop-aws:3.3.4",
        "com.amazonaws:aws-java-sdk-bundle:1.12.262"
    ]
    
    builder = SparkSession.builder \
        .appName("WeatherLambdaArchitecture") \
        .master("local[*]") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.driver.memory", "1g") \
        .config("spark.executor.memory", "1g") \
        .config("spark.jars.packages", ",".join(packages)) \
        # --- FIX Lá»–I KUBERNETES ---
        # Chuyá»ƒn thÆ° má»¥c cache cá»§a Ivy (nÆ¡i lÆ°u file JAR) vá» /tmp
        # VÃ¬ user 1001 trong Docker Ä‘Ã´i khi khÃ´ng cÃ³ quyá»n ghi vÃ o thÆ° má»¥c home máº·c Ä‘á»‹nh
        .config("spark.jars.ivy", "/tmp/.ivy2") 

    print("ğŸ“¦ Äang náº¡p cáº¥u hÃ¬nh MinIO S3...")
    for key, value in minio_config.SPARK_S3_CONFIG.items():
        builder = builder.config(key, value)
    
    spark = builder.getOrCreate()
    spark.sparkContext.setLogLevel("WARN")
    return spark

def run_etl_pipeline():
    print("\n" + "="*80)
    print("ğŸš€ SPARK LAMBDA ON K8S: KAFKA -> MINIO & REDIS")
    print("="*80)
    
    # 1. Khá»Ÿi táº¡o Spark
    try:
        spark = create_spark_session()
        print("âœ… Spark Session Ä‘Ã£ khá»Ÿi táº¡o thÃ nh cÃ´ng!")
    except Exception as e:
        print(f"âŒ Lá»—i khá»Ÿi táº¡o Spark: {e}")
        sys.exit(1)
    
    # 2. READ (Äá»c tá»« Kafka qua Service K8s)
    # LÆ°u Ã½: kafka_config Ä‘Ã£ Ä‘Æ°á»£c update Ä‘á»ƒ láº¥y env KAFKA_BOOTSTRAP_SERVERS
    reader = DataReader(spark, source_type="kafka", kafka_mode="streaming")
    cleaner = DataCleaner()
    normalizer = DataNormalizer()
    enricher = DataEnricher()
    
    print(f"\nğŸ§ Äang Ä‘á»c topic '{kafka_config.KAFKA_TOPICS['weather']}'...")
    weather_df = reader.read_weather_data()
    
    # 3. TRANSFORM
    print("âš™ï¸  Äang xá»­ lÃ½ dá»¯ liá»‡u...")
    weather_clean = cleaner.clean_weather_data(weather_df)
    weather_norm = normalizer.normalize_weather_data(weather_clean)
    weather_enriched = enricher.enrich_with_disaster_risk(weather_norm)
    
    # ==========================================
    # NHÃNH 1: BATCH LAYER -> MINIO
    # ==========================================
    output_minio = minio_config.get_minio_path("enriched", "weather", format="parquet")
    # Checkpoint lÆ°u trÃªn MinIO Ä‘á»ƒ Ä‘áº£m báº£o an toÃ n náº¿u Pod cháº¿t
    ckpt_minio = f"s3a://{minio_config.MINIO_BUCKET}/checkpoints/weather_minio"
    
    print(f"\nğŸ’¾ MinIO Writer (Batch): {output_minio}")
    
    query_minio = weather_enriched.writeStream \
        .outputMode("append") \
        .format("parquet") \
        .option("path", output_minio) \
        .option("checkpointLocation", ckpt_minio) \
        .trigger(processingTime="30 seconds") \
        .queryName("WriteToMinIO") \
        .start()

    # ==========================================
    # NHÃNH 2: SPEED LAYER -> REDIS
    # ==========================================
    redis_writer = RedisWriter()
    # Checkpoint khÃ¡c biá»‡t cho Redis
    ckpt_redis = f"s3a://{minio_config.MINIO_BUCKET}/checkpoints/weather_redis"
    
    print(f"\nğŸ”¥ Redis Writer (Realtime): weather-redis")
    
    query_redis = weather_enriched.writeStream \
        .outputMode("append") \
        .foreachBatch(redis_writer.write_stream_to_redis) \
        .option("checkpointLocation", ckpt_redis) \
        .trigger(processingTime="5 seconds") \
        .queryName("WriteToRedis") \
        .start()

    print("\n" + "="*80)
    print("âœ… PIPELINE ÄANG CHáº Y TRONG KUBERNETES POD")
    print("ğŸ‘‰ DÃ¹ng lá»‡nh 'kubectl logs -f [tÃªn-pod]' Ä‘á»ƒ theo dÃµi")
    print("="*80)

    try:
        spark.streams.awaitAnyTermination()
    except KeyboardInterrupt:
        print("\nğŸ›‘ Äang dá»«ng pipeline...")
        spark.stop()

if __name__ == "__main__":
    run_etl_pipeline()