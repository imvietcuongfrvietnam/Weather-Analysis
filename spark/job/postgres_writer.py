"""
PostgreSQL Writer - Ghi d·ªØ li·ªáu d·ª± ƒëo√°n v√†o PostgreSQL
Save weather forecast predictions to PostgreSQL database
Updated: Changed mode to 'overwrite' to handle schema changes automatically.
"""

from pyspark.sql import DataFrame
import sys
import os

# --- IMPORT CONFIG ---
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    import config
except ImportError:
    class Config:
        # Fallback Config n·∫øu kh√¥ng import ƒë∆∞·ª£c
        POSTGRES_JDBC_URL = "jdbc:postgresql://weather-postgresql.default.svc.cluster.local:5432/weather_db"
        POSTGRES_PROPERTIES = {
            "user": "weather_user",
            "password": "weather_pass",
            "driver": "org.postgresql.Driver"
        }
        # QUAN TR·ªåNG: D√πng overwrite ƒë·ªÉ t·ª± ƒë·ªông recreate table khi schema thay ƒë·ªïi
        POSTGRES_TABLE = "weather_predictions"
    config = Config()

class PostgresWriter:
    """Write forecast predictions to PostgreSQL database"""
    
    def __init__(self):
        self.jdbc_url = getattr(config, 'POSTGRES_JDBC_URL', None)
        self.properties = getattr(config, 'POSTGRES_PROPERTIES', {})
        self.table = getattr(config, 'POSTGRES_TABLE', 'weather_predictions')
        
        # ‚ö†Ô∏è CHUY·ªÇN SANG 'overwrite':
        # Spark s·∫Ω Drop table c≈© v√† Create table m·ªõi kh·ªõp v·ªõi DataFrame.
        # Gi√∫p gi·∫£i quy·∫øt l·ªói "Column not found" t·ª± ƒë·ªông.
        self.write_mode = "overwrite" 
        
    def write_predictions(self, df: DataFrame, table_name: str = None):
        """
        Ghi DataFrame d·ª± ƒëo√°n v√†o PostgreSQL
        """
        if not self.jdbc_url:
            print("‚ùå Cannot write: Missing PostgreSQL configuration.")
            return False

        table = table_name or self.table
        
        print(f"\nüíæ Writing predictions to PostgreSQL...")
        print(f"   URL: {self.jdbc_url}")
        print(f"   Table: {table}")
        print(f"   Mode: {self.write_mode} (Will drop & recreate table)")
        
        try:
            # Ghi v√†o PostgreSQL qua JDBC
            df.write \
                .jdbc(
                    url=self.jdbc_url,
                    table=table,
                    mode=self.write_mode, # overwrite
                    properties=self.properties
                )
            
            print(f"   ‚úÖ Successfully wrote records to PostgreSQL")
            return True
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è  PostgreSQL Write Failed: {e}")
            print(f"   üí° Nguy√™n nh√¢n: {e}")
            return False
    
    def write_predictions_safe(self, df: DataFrame, table_name: str = None):
        """
        Phi√™n b·∫£n an to√†n: Kh√¥ng g√¢y crash ch∆∞∆°ng tr√¨nh n·∫øu l·ªói DB
        """
        try:
            return self.write_predictions(df, table_name)
        except Exception:
            return False
    
    def create_table_sql(self) -> str:
        """
        SQL tham kh·∫£o (Spark overwrite s·∫Ω t·ª± l√†m b∆∞·ªõc n√†y, nh∆∞ng gi·ªØ l·∫°i ƒë·ªÉ debug)
        ƒê√£ c·∫≠p nh·∫≠t th√™m wind_direction
        """
        sql = f"""
CREATE TABLE IF NOT EXISTS {self.table} (
    datetime TIMESTAMP,
    city VARCHAR(100),
    
    -- Actual values
    temperature DOUBLE PRECISION,
    humidity DOUBLE PRECISION,
    pressure DOUBLE PRECISION,
    wind_speed DOUBLE PRECISION,
    wind_direction DOUBLE PRECISION, -- ‚úÖ M·ªõi th√™m
    
    -- Predicted values
    prediction_temperature DOUBLE PRECISION,
    prediction_humidity DOUBLE PRECISION,
    prediction_pressure DOUBLE PRECISION,
    prediction_wind_speed DOUBLE PRECISION,
    prediction_wind_direction DOUBLE PRECISION, -- ‚úÖ M·ªõi th√™m
    prediction_weather_desc VARCHAR(100),
    
    created_at TIMESTAMP
);
        """
        return sql.strip()

if __name__ == "__main__":
    writer = PostgresWriter()
    print("--- SQL SCHEMA REFERENCE ---")
    print(writer.create_table_sql())