"""
Models - ML Models for Weather Forecasting
C√°c m√¥ h√¨nh ML cho d·ª± ƒëo√°n t·ª´ng ƒë·∫∑c tr∆∞ng th·ªùi ti·∫øt
"""

from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer, IndexToString
from pyspark.ml.regression import GBTRegressor, RandomForestRegressor
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml import Pipeline, PipelineModel
from pyspark.sql import DataFrame
import sys
import os

# --- IMPORT CONFIG ---
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
try:
    import config
except ImportError:
    class Config:
        CONTINUOUS_FEATURES = ["temperature", "humidity", "pressure", "wind_speed", "precipitation_mm"]
        CATEGORICAL_FEATURES = ["weather_desc"]
        GBT_PARAMS = {'maxIter': 20, 'maxDepth': 5, 'stepSize': 0.1, 'subsamplingRate': 0.8}
        RF_REGRESSION_PARAMS = {'numTrees': 20, 'maxDepth': 5, 'minInstancesPerNode': 2, 'subsamplingRate': 0.8}
        RF_CLASSIFICATION_PARAMS = {'numTrees': 20, 'maxDepth': 5, 'minInstancesPerNode': 2}
        MODEL_SELECTION = {}
        RANDOM_SEED = 42
    config = Config()

class WeatherForecastModels:
    """Build and manage forecasting models for different weather features"""
    
    def __init__(self):
        self.models = {}
        self.feature_cols = []
        
    def build_regression_model(self, target_feature: str, feature_cols: list, model_type: str = "GBT"):
        """
        X√¢y d·ª±ng Pipeline cho b√†i to√°n h·ªìi quy (D·ª± ƒëo√°n s·ªë)
        """
        print(f"\nü§ñ Building {model_type} model for {target_feature}...")
        
        # T·∫°o t√™n c·ªôt feature ri√™ng bi·ªát ƒë·ªÉ tr√°nh xung ƒë·ªôt khi ch·∫°y nhi·ªÅu model
        features_raw_col = f"features_raw_{target_feature}"
        features_scaled_col = f"features_{target_feature}"
        
        # 1. Gom c√°c c·ªôt features th√†nh 1 vector
        assembler = VectorAssembler(
            inputCols=feature_cols,
            outputCol=features_raw_col,
            handleInvalid="skip" 
        )
        
        # 2. Chu·∫©n h√≥a d·ªØ li·ªáu
        scaler = StandardScaler(
            inputCol=features_raw_col,
            outputCol=features_scaled_col,
            withStd=True,
            withMean=True
        )
        
        # 3. Ch·ªçn thu·∫≠t to√°n
        prediction_col = f"prediction_{target_feature}"
        
        if model_type == "GBT":
            model = GBTRegressor(
                featuresCol=features_scaled_col,
                labelCol=target_feature,
                predictionCol=prediction_col,
                maxIter=config.GBT_PARAMS['maxIter'],
                maxDepth=config.GBT_PARAMS['maxDepth'],
                stepSize=config.GBT_PARAMS['stepSize'],
                seed=config.RANDOM_SEED
            )
        else:
            model = RandomForestRegressor(
                featuresCol=features_scaled_col,
                labelCol=target_feature,
                predictionCol=prediction_col,
                numTrees=config.RF_REGRESSION_PARAMS['numTrees'],
                maxDepth=config.RF_REGRESSION_PARAMS['maxDepth'],
                seed=config.RANDOM_SEED
            )
        
        pipeline = Pipeline(stages=[assembler, scaler, model])
        return pipeline
    
    def build_classification_model(self, target_feature: str, feature_cols: list):
        """
        X√¢y d·ª±ng Pipeline cho b√†i to√°n ph√¢n lo·∫°i (D·ª± ƒëo√°n Category)
        """
        print(f"\nüè∑Ô∏è  Building classifier for {target_feature}...")
        
        features_raw_col = f"features_raw_{target_feature}"
        features_scaled_col = f"features_{target_feature}"
        
        # 1. String Indexer
        label_indexer = StringIndexer(
            inputCol=target_feature,
            outputCol="label", 
            handleInvalid="skip"
        )
        
        # 2. Vector Assembler
        assembler = VectorAssembler(
            inputCols=feature_cols,
            outputCol=features_raw_col,
            handleInvalid="skip"
        )
        
        # 3. Scaler
        scaler = StandardScaler(
            inputCol=features_raw_col,
            outputCol=features_scaled_col,
            withStd=True,
            withMean=True
        )
        
        # 4. Classifier
        classifier = RandomForestClassifier(
            featuresCol=features_scaled_col,
            labelCol="label",
            predictionCol="prediction_indexed",
            numTrees=config.RF_CLASSIFICATION_PARAMS['numTrees'],
            maxDepth=config.RF_CLASSIFICATION_PARAMS['maxDepth'],
            seed=config.RANDOM_SEED
        )
        
        # 5. IndexToString
        label_converter = IndexToString(
            inputCol="prediction_indexed",
            outputCol=f"prediction_{target_feature}"
        )
        
        pipeline = Pipeline(stages=[label_indexer, assembler, scaler, classifier, label_converter])
        return pipeline
    
    def build_all_models(self, feature_cols: list):
        """
        X√¢y d·ª±ng to√†n b·ªô c√°c models c·∫ßn thi·∫øt
        """
        print("\n" + "="*60)
        print("üèóÔ∏è  BUILDING ALL FORECAST MODELS")
        print("="*60)
        
        models = {}
        
        # Regression Models
        for target in config.CONTINUOUS_FEATURES:
            model_type = config.MODEL_SELECTION.get(target, "GBT")
            models[target] = self.build_regression_model(target, feature_cols, model_type)
        
        # Classification Models
        if hasattr(config, 'CATEGORICAL_FEATURES'):
            for target in config.CATEGORICAL_FEATURES:
                models[target] = self.build_classification_model(target, feature_cols)
        
        self.models = models
        self.feature_cols = feature_cols
        
        print(f"\n‚úÖ Built {len(models)} models successfully!")
        print("="*60 + "\n")
        
        return models
    
    def train_all_models(self, train_df: DataFrame):
        """
        Hu·∫•n luy·ªán to√†n b·ªô c√°c models
        """
        print("\n" + "="*60)
        print("üéì TRAINING ALL MODELS")
        print("="*60)
        
        if not self.models:
            raise ValueError("Models not built yet. Call build_all_models() first.")
        
        trained_models = {}
        
        for target_feature, pipeline in self.models.items():
            print(f"\nüöÇ Training model for {target_feature}...")
            try:
                # Ch·ªâ train tr√™n c√°c d√≤ng c√≥ d·ªØ li·ªáu label
                train_data = train_df.filter(train_df[target_feature].isNotNull())
                
                # Fit model
                model = pipeline.fit(train_data)
                trained_models[target_feature] = model
                print(f"   ‚úÖ Training complete for {target_feature}")
                
            except Exception as e:
                print(f"   ‚ùå Error training {target_feature}: {e}")
                import traceback
                traceback.print_exc()
        
        return trained_models
    
    def save_all_models(self, trained_models: dict, base_path: str):
        """
        L∆∞u c√°c model ƒë√£ train xu·ªëng ·ªï c·ª©ng
        """
        print(f"\nüíæ Saving models to {base_path}...")
        
        for target_feature, model in trained_models.items():
            try:
                model_path = os.path.join(base_path, f"{target_feature}_model")
                model.write().overwrite().save(model_path)
                print(f"   Saved: {model_path}")
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Could not save model for {target_feature}: {e}")

if __name__ == "__main__":
    print("Models module loaded.")