"""
Data Loader - Load weather data from MinIO
ƒê·ªçc d·ªØ li·ªáu th·ªùi ti·∫øt ƒë√£ ƒë∆∞·ª£c l√†m s·∫°ch v√† chu·∫©n h√≥a (Output c·ªßa main_etl.py)
"""

from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import col, count, when, isnan, to_timestamp, min as spark_min, max as spark_max, avg, stddev
from pyspark.sql.types import TimestampType
import sys
import os

# Th√™m ƒë∆∞·ªùng d·∫´n ƒë·ªÉ import config
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    import config
except ImportError:
    # Fallback Config n·∫øu kh√¥ng t√¨m th·∫•y file config.py
    class Config:
        MINIO_BUCKET = "weather-data"
        # ƒê∆∞·ªùng d·∫´n t·ªõi d·ªØ li·ªáu ƒë·∫ßu ra c·ªßa qu√° tr√¨nh Normalization
        MINIO_INPUT_PATH = f"s3a://{MINIO_BUCKET}/enriched/weather" 
        
        # C√°c c·ªôt m·ª•c ti√™u quan tr·ªçng (C·∫≠p nh·∫≠t m·ªõi nh·∫•t)
        ALL_TARGET_FEATURES = [
            "temperature", 
            "humidity", 
            "pressure", 
            "wind_speed", 
            "wind_direction"
        ]
        
        # C√°c c·ªôt s·ªë li√™n t·ª•c
        CONTINUOUS_FEATURES = ALL_TARGET_FEATURES
        
        # Ng∆∞·ª°ng ki·ªÉm tra ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu
        MAX_MISSING_PCT = 0.05       # 5%
        MIN_DAYS_HISTORY = 1         # T·ªëi thi·ªÉu 1 ng√†y
        MIN_TRAINING_RECORDS = 50    # T·ªëi thi·ªÉu 50 d√≤ng
        
    config = Config()

class WeatherDataLoader:
    """Load and validate weather data from MinIO"""
    
    def __init__(self, spark: SparkSession):
        self.spark = spark
        # M·∫∑c ƒë·ªãnh load t·ª´ folder enriched/weather (N∆°i MinIOWriter ghi Parquet)
        if hasattr(config, 'MINIO_INPUT_PATH'):
            self.input_path = config.MINIO_INPUT_PATH
        else:
            self.input_path = "s3a://weather-data/enriched/weather"
        
    def load_data(self, city: str = None, limit_rows: int = None) -> DataFrame:
        """
        Load weather data (Parquet) from MinIO
        """
        print(f"\nüìÇ Loading data from: {self.input_path}")
        
        try:
            # 1. ƒê·ªçc t·∫•t c·∫£ file Parquet t·ª´ MinIO
            # mergeSchema=True gi√∫p ƒë·ªçc ƒë∆∞·ª£c nhi·ªÅu file d√π schema c√≥ thay ƒë·ªïi nh·ªè
            df = self.spark.read.option("mergeSchema", "true").parquet(self.input_path)
            
            total_count = df.count()
            print(f"   ‚úÖ Successfully loaded {total_count} records")
            
            if total_count == 0:
                print("   ‚ö†Ô∏è  Warning: Dataset is empty!")
                return df
            
            # 2. L·ªçc theo th√†nh ph·ªë (n·∫øu c√≥ y√™u c·∫ßu)
            if city:
                df = df.filter(col("city") == city)
                print(f"   üèôÔ∏è  Filtered to city: {city} ({df.count()} records)")
            
            # 3. Chu·∫©n h√≥a c·ªôt Datetime
            if "datetime" not in df.columns:
                print("   ‚ö†Ô∏è  Column 'datetime' not found! Trying to find alternative...")
                raise ValueError("Column 'datetime' not found in data!")
            
            # √âp ki·ªÉu sang Timestamp n·∫øu n√≥ ƒëang l√† String
            if not isinstance(df.schema["datetime"].dataType, TimestampType):
                df = df.withColumn("datetime", to_timestamp(col("datetime")))
            
            # 4. S·∫Øp x·∫øp theo th·ªùi gian (Quan tr·ªçng cho Time Series)
            df = df.orderBy("datetime")
            
            # 5. Gi·ªõi h·∫°n s·ªë d√≤ng (cho m·ª•c ƒë√≠ch test nhanh)
            if limit_rows:
                df = df.limit(limit_rows)
                print(f"   ‚ö†Ô∏è  Limited to {limit_rows} records for testing")
            
            return df
            
        except Exception as e:
            print(f"   ‚ùå Error loading data: {e}")
            # Tr·∫£ v·ªÅ DataFrame r·ªóng thay v√¨ crash ch∆∞∆°ng tr√¨nh
            from pyspark.sql.types import StructType
            return self.spark.createDataFrame([], StructType([]))
    
    def validate_data(self, df: DataFrame) -> dict:
        """
        Ki·ªÉm tra ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu (Data Quality Check)
        """
        print("\nüîç Validating data quality...")
        
        if df.rdd.isEmpty():
            print("   ‚ùå Validation Failed: DataFrame is empty")
            return {'quality_score': 0}

        total_records = df.count()
        validation_results = {
            'total_records': total_records,
            'missing_values': {},
            'data_range': {},
            'quality_score': 100.0
        }
        
        # 1. Ki·ªÉm tra c√°c c·ªôt b·∫Øt bu·ªôc (Features)
        required_features = config.ALL_TARGET_FEATURES
        missing_features = [f for f in required_features if f not in df.columns]
        
        if missing_features:
            print(f"   ‚ö†Ô∏è  Missing required features: {missing_features}")
            validation_results['missing_features'] = missing_features
            validation_results['quality_score'] -= 20
        
        # 2. Ki·ªÉm tra gi√° tr·ªã thi·∫øu (Missing Values)
        print("   üìä Checking missing values...")
        features_to_check = [f for f in config.ALL_TARGET_FEATURES if f in df.columns]
        
        for feature in features_to_check:
            null_count = df.filter(col(feature).isNull() | isnan(col(feature))).count()
            null_pct = (null_count / total_records) * 100
            
            validation_results['missing_values'][feature] = {
                'count': null_count,
                'percentage': null_pct
            }
            
            if null_pct > config.MAX_MISSING_PCT * 100:
                print(f"      ‚ö†Ô∏è  {feature}: {null_pct:.2f}% missing (High!)")
                validation_results['quality_score'] -= 5
        
        # 3. Ki·ªÉm tra d·∫£i d·ªØ li·ªáu (Data Range) - Ph√°t hi·ªán Outliers
        print("   üìà Checking data ranges...")
        # L·ªçc c√°c c·ªôt s·ªë th·ª±c t·∫ø c√≥ trong DF
        numeric_cols = [f for f in config.CONTINUOUS_FEATURES if f in df.columns]
        
        if numeric_cols:
            # T√≠nh min/max m·ªôt l·∫ßn cho nhanh
            aggregations = []
            for col_name in numeric_cols:
                aggregations.append(spark_min(col_name).alias(f"min_{col_name}"))
                aggregations.append(spark_max(col_name).alias(f"max_{col_name}"))
            
            stats = df.agg(*aggregations).collect()[0]
            
            for feature in numeric_cols:
                min_val = stats[f"min_{feature}"]
                max_val = stats[f"max_{feature}"]
                
                validation_results['data_range'][feature] = {'min': min_val, 'max': max_val}
                print(f"      {feature}: [{min_val:.2f}, {max_val:.2f}]")
        
        # 4. Ki·ªÉm tra kho·∫£ng th·ªùi gian (Time Span)
        time_stats = df.agg(spark_min('datetime'), spark_max('datetime')).collect()[0]
        start_time = time_stats[0]
        end_time = time_stats[1]
        
        if start_time and end_time:
            time_span_days = (end_time - start_time).days
            validation_results['time_span_days'] = time_span_days
            print(f"\n   üìÖ Time span: {time_span_days} days ({start_time} to {end_time})")
            
            if time_span_days < config.MIN_DAYS_HISTORY:
                print(f"   ‚ö†Ô∏è  Warning: Only {time_span_days} days of data (Need more history for ML)")
                validation_results['quality_score'] -= 15
        
        # 5. Ki·ªÉm tra s·ªë l∆∞·ª£ng b·∫£n ghi t·ªëi thi·ªÉu
        if total_records < config.MIN_TRAINING_RECORDS:
            print(f"   ‚ùå Insufficient data: {total_records} records (minimum: {config.MIN_TRAINING_RECORDS})")
            validation_results['quality_score'] -= 30
        
        # K·∫øt lu·∫≠n
        quality_score = max(0, validation_results['quality_score']) # Kh√¥ng √¢m
        status = "GOOD" if quality_score >= 80 else ("ACCEPTABLE" if quality_score >= 60 else "POOR")
        print(f"\n   ‚úÖ Data Quality Score: {quality_score:.1f}% - {status}")
        
        return validation_results
    
    def get_cities(self, df: DataFrame) -> list:
        """L·∫•y danh s√°ch th√†nh ph·ªë c√≥ trong d·ªØ li·ªáu"""
        if 'city' in df.columns:
            return [row.city for row in df.select('city').distinct().collect()]
        return []
    
    def summary_stats(self, df: DataFrame):
        """In th·ªëng k√™ t√≥m t·∫Øt"""
        print("\n" + "="*60)
        print("üìä DATA SUMMARY")
        print("="*60)
        
        if df.rdd.isEmpty():
            print("   (Empty DataFrame)")
            return

        print(f"Total Records:     {df.count()}")
        print(f"Total Columns:     {len(df.columns)}")
        
        cities = self.get_cities(df)
        print(f"Cities ({len(cities)}):      {', '.join(cities[:5])}...")
        
        print("\nüìà Sample Data (Top 5):")
        # Ch·ªçn c√°c c·ªôt quan tr·ªçng ƒë·ªÉ hi·ªÉn th·ªã
        display_cols = ['datetime', 'city'] + [c for c in config.ALL_TARGET_FEATURES if c in df.columns]
        df.select(display_cols).show(5, truncate=False)
        
        print("="*60 + "\n")

if __name__ == "__main__":
    print("Testing data loader module...")