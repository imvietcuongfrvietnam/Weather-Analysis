"""
Weather Forecasting - Main Pipeline
D·ª± ƒëo√°n th·ªùi ti·∫øt s·ª≠ d·ª•ng Spark ML v√† d·ªØ li·ªáu t·ª´ MinIO
Updated: Added Future Forecast Generation (7 Days) & Hardcoded MinIO
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit, date_add, expr
import sys
import os
import argparse
from datetime import datetime

# --- IMPORT MODULES ---
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    import config
    from visualization import ForecastVisualizer
    from data_loader import WeatherDataLoader
    from feature_engineering import TimeSeriesFeatureEngineer
    from models import WeatherForecastModels
    from forecast_evaluator import ForecastEvaluator
    from postgres_writer import PostgresWriter
except ImportError as e:
    print(f"‚ùå L·ªói Import: {e}")
    print("üí° ƒê·∫£m b·∫£o b·∫°n ƒëang ch·∫°y file n√†y t·ª´ th∆∞ m·ª•c spark/job/ ho·∫∑c ƒë√£ setup PYTHONPATH ƒë√∫ng.")
    sys.exit(1)

# C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n l∆∞u Model/Output c·ª•c b·ªô
LOCAL_MODEL_DIR = "./models_output"
LOCAL_OUTPUT_DIR = "./predictions_output"
TRAIN_TEST_SPLIT = 0.8

def create_spark_session():
    """
    Kh·ªüi t·∫°o Spark Session v·ªõi c·∫•u h√¨nh MinIO S3A + PostgreSQL Driver
    """
    print("\n" + "="*80)
    print("üöÄ WEATHER FORECASTING ML SYSTEM")
    print("="*80)
    print("‚ö° Initializing Spark Session...")
    
    # S·ª≠ d·ª•ng c√°c version ƒë√£ ki·ªÉm ch·ª©ng ho·∫°t ƒë·ªông ·ªïn ƒë·ªãnh
    packages = [
        "org.apache.hadoop:hadoop-aws:3.3.2",
        "com.amazonaws:aws-java-sdk-bundle:1.11.1026",
        "org.postgresql:postgresql:42.5.0"
    ]
    
    builder = SparkSession.builder \
        .appName("WeatherForecast_Training") \
        .master("local[*]") \
        .config("spark.driver.memory", "1g") \
        .config("spark.executor.memory", "1g") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.jars.packages", ",".join(packages)) \
        .config("spark.jars.ivy", "/tmp/.ivy2")

    # =========================================================================
    # üõ† FIX C·ª®NG: C·∫§U H√åNH MINIO TR·ª∞C TI·∫æP ƒê·ªÇ TR√ÅNH L·ªñI DNS (UnknownHostException)
    # =========================================================================
    print("üîí Applying HARDCODED MinIO Configuration...")
    builder = builder \
        .config("spark.hadoop.fs.s3a.endpoint", "http://weather-minio.default.svc.cluster.local:9000") \
        .config("spark.hadoop.fs.s3a.access.key", "admin") \
        .config("spark.hadoop.fs.s3a.secret.key", "password123") \
        .config("spark.hadoop.fs.s3a.path.style.access", "true") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false")
    # =========================================================================
    
    spark = builder.getOrCreate()
    spark.sparkContext.setLogLevel("WARN")
    
    print("‚úÖ Spark Session initialized successfully!")
    return spark

def generate_future_forecast(spark, model_dict, last_known_data, days=7):
    """
    Sinh ra d·ª± b√°o cho 7 ng√†y ti·∫øp theo d·ª±a tr√™n d·ªØ li·ªáu ng√†y cu·ªëi c√πng (Persistence Strategy)
    """
    print(f"\nüîÆ Generating forecast for next {days} days...")
    
    # 1. L·∫•y d√≤ng d·ªØ li·ªáu cu·ªëi c√πng l√†m c∆° s·ªü
    # S·∫Øp x·∫øp gi·∫£m d·∫ßn theo th·ªùi gian v√† l·∫•y 1 d√≤ng ƒë·∫ßu ti√™n
    last_row = last_known_data.orderBy(col("datetime").desc()).limit(1)
    
    if last_row.count() == 0:
        print("‚ö†Ô∏è No data to generate forecast from.")
        return None

    future_preds = []
    
    # 2. V√≤ng l·∫∑p sinh 7 ng√†y
    # Gi·∫£ ƒë·ªãnh: C√°c y·∫øu t·ªë ƒë·∫ßu v√†o (features) cho ng√†y mai t∆∞∆°ng t·ª± ng√†y h√¥m nay (Persistence)
    # M√¥ h√¨nh s·∫Ω d√πng input ƒë√≥ ƒë·ªÉ predict ra output.
    for i in range(1, days + 1):
        # T·∫°o ng√†y t∆∞∆°ng lai: last_date + i
        next_day = last_row.withColumn("datetime", date_add(col("datetime"), i))
        
        row_dict = {}
        # L·∫•y datetime v√† city ra ƒë·ªÉ l∆∞u
        row_dict['datetime'] = next_day.select("datetime").collect()[0][0]
        row_dict['city'] = next_day.select("city").collect()[0][0]
        
        # D·ª± ƒëo√°n t·ª´ng ch·ªâ s·ªë b·∫±ng c√°c model ƒë√£ train
        for target, model in model_dict.items():
            # D·ª± ƒëo√°n
            pred_df = model.transform(next_day)
            # L·∫•y gi√° tr·ªã prediction
            # C·ªôt output c·ªßa model th∆∞·ªùng t√™n l√† f"prediction_{target}" ho·∫∑c "prediction" t√πy config
            # Trong file models.py ta ƒë√£ set outputCol=f"prediction_{target}"
            pred_col_name = f"prediction_{target}"
            
            if pred_col_name in pred_df.columns:
                val = pred_df.select(pred_col_name).collect()[0][0]
                row_dict[pred_col_name] = val
            
        future_preds.append(row_dict)
        
    # 3. T·∫°o DataFrame t·ª´ list d·ª± b√°o
    if not future_preds:
        return None
        
    future_df = spark.createDataFrame(future_preds)
    print(f"   ‚úÖ Generated {days} future days.")
    return future_df

def run_forecasting_pipeline(city: str = None, limit_rows: int = None, save_models: bool = True):
    
    spark = create_spark_session()
    
    try:
        # ==========================================
        # STEP 1: LOAD DATA FROM MINIO
        # ==========================================
        print("\n" + "="*80)
        print("STEP 1: LOADING DATA FROM MINIO")
        print("="*80)
        
        loader = WeatherDataLoader(spark)
        df = loader.load_data(city=city, limit_rows=limit_rows)
        
        if df is None or df.rdd.isEmpty():
            print("‚ùå ERROR: Dataframe is empty! Please check if Streaming Job has written data to MinIO.")
            return

        validation = loader.validate_data(df)
        if validation['quality_score'] < 50:
            print("‚ö†Ô∏è Data quality too poor. Exiting.")
            return

        loader.summary_stats(df)
        
        # ==========================================
        # STEP 2: FEATURE ENGINEERING
        # ==========================================
        print("\n" + "="*80)
        print("STEP 2: FEATURE ENGINEERING")
        print("="*80)
        
        engineer = TimeSeriesFeatureEngineer()
        df_features = engineer.engineer_all_features(df)
        
        feature_cols = engineer.get_feature_columns(df_features, exclude_targets=True)
        print(f"\nüìä Total features created: {len(feature_cols)}")
        
        # ==========================================
        # STEP 3: TRAIN/TEST SPLIT
        # ==========================================
        print("\n" + "="*80)
        print("STEP 3: SPLITTING DATA")
        print("="*80)
        
        df_clean = df_features.dropna()
        train_df, test_df = df_clean.randomSplit([TRAIN_TEST_SPLIT, 1 - TRAIN_TEST_SPLIT], seed=42)
        
        print(f"Training set:   {train_df.count()} rows")
        print(f"Test set:       {test_df.count()} rows")
        
        if train_df.count() < 50:
            print("‚ùå Not enough data to train. Need at least 50 rows.")
            return

        # ==========================================
        # STEP 4 & 5: BUILD & TRAIN MODELS
        # ==========================================
        print("\n" + "="*80)
        print("STEP 4 & 5: BUILDING & TRAINING MODELS")
        print("="*80)
        
        model_builder = WeatherForecastModels()
        model_builder.build_all_models(feature_cols)
        trained_models = model_builder.train_all_models(train_df)
        
        if save_models:
            print(f"\nüíæ Saving models to {LOCAL_MODEL_DIR}...")
            if not os.path.exists(LOCAL_MODEL_DIR):
                os.makedirs(LOCAL_MODEL_DIR)
            model_builder.save_all_models(trained_models, LOCAL_MODEL_DIR)
        
        # ==========================================
        # STEP 6: EVALUATE (Test Set - Historical)
        # ==========================================
        print("\n" + "="*80)
        print("STEP 6: EVALUATION (HISTORICAL)")
        print("="*80)
        
        predictions_test_df = test_df
        for target, model in trained_models.items():
            predictions_test_df = model.transform(predictions_test_df)
            
        evaluator = ForecastEvaluator()
        metrics = evaluator.evaluate_all_models(predictions_test_df)
        
        print("\nüìä Evaluation Summary:")
        for target, m in metrics.items():
            print(f"   - {target}: RMSE={m.get('rmse', 'N/A'):.4f}, R2={m.get('r2', 'N/A'):.4f}")

        # ==========================================
        # STEP 6.5: GENERATE FUTURE FORECAST (7 DAYS)
        # ==========================================
        print("\n" + "="*80)
        print("STEP 6.5: GENERATING FUTURE FORECAST")
        print("="*80)
        
        # D√πng df_features (to√†n b·ªô d·ªØ li·ªáu) ƒë·ªÉ l·∫•y d√≤ng cu·ªëi c√πng l√†m m·ªëc
        future_forecast_df = generate_future_forecast(spark, trained_models, df_features, days=7)

        # ==========================================
        # STEP 7: WRITE TO POSTGRESQL 
        # ==========================================
        print("\n" + "="*80)
        print("STEP 7: WRITING TO POSTGRESQL")
        print("="*80)

        # 1. X√°c ƒë·ªãnh c·ªôt c·∫ßn ghi
        target_cols = list(config.CONTINUOUS_FEATURES) 
        if hasattr(config, 'CATEGORICAL_FEATURES'):
            target_cols += config.CATEGORICAL_FEATURES            
        prediction_cols = [f"prediction_{c}" for c in target_cols]
        
        # C·ªôt c∆° b·∫£n
        base_cols = ['datetime', 'city']
        
        # --- X·ª≠ l√Ω DataFrame Test (Qu√° kh·ª©) ---
        # Select: Time + City + Actual + Prediction
        select_cols_test = base_cols + \
                           [c for c in target_cols if c in predictions_test_df.columns] + \
                           [c for c in prediction_cols if c in predictions_test_df.columns]
        
        export_test_df = predictions_test_df.select(select_cols_test)
        
        # --- X·ª≠ l√Ω DataFrame Future (T∆∞∆°ng lai) ---
        # Future DF ch·ªâ c√≥ c·ªôt Prediction. Ta c·∫ßn th√™m c·ªôt Actual (l√† Null) ƒë·ªÉ union ƒë∆∞·ª£c
        if future_forecast_df:
            for col_name in target_cols:
                # Th√™m c·ªôt Actual v·ªõi gi√° tr·ªã Null (v√¨ t∆∞∆°ng lai ch∆∞a x·∫£y ra)
                future_forecast_df = future_forecast_df.withColumn(col_name, lit(None).cast("double"))
            
            # Select ƒë√∫ng th·ª© t·ª± c·ªôt nh∆∞ Test DF
            # L∆∞u √Ω: future_forecast_df c√≥ th·ªÉ thi·∫øu m·ªôt s·ªë c·ªôt n·∫øu model kh√¥ng predict ra, c·∫ßn check
            valid_future_cols = [c for c in select_cols_test if c in future_forecast_df.columns]
            export_future_df = future_forecast_df.select(valid_future_cols)
            
            # Union: G·ªôp Qu√° kh·ª© + T∆∞∆°ng lai
            final_export_df = export_test_df.unionByName(export_future_df, allowMissingColumns=True)
        else:
            final_export_df = export_test_df

        print(f"   Writing {final_export_df.count()} records (Historical + Future) to database...")
        
        # 2. G·ªçi Postgres Writer
        pg_writer = PostgresWriter()
        success = pg_writer.write_predictions_safe(final_export_df)
        
        if success:
            print("   ‚úÖ Database update complete.")
        else:
            print("   ‚ö†Ô∏è Database update skipped/failed.")

        # ==========================================
        # STEP 8: EXPORT CSV & VISUALIZATION
        # ==========================================
        if not os.path.exists(LOCAL_OUTPUT_DIR):
            os.makedirs(LOCAL_OUTPUT_DIR)
            
        print(f"\n‚úÖ Pipeline Complete.")
        
        print("\n" + "="*80)
        print("STEP 8: VISUALIZATION")
        print("="*80)

        try:
            viz = ForecastVisualizer()
            # Chuy·ªÉn Spark DataFrame sang Pandas ƒë·ªÉ v·∫Ω (Ch·ªâ l√†m khi d·ªØ li·ªáu < 100k d√≤ng)
            pandas_df = predictions_test_df.toPandas()
            
            # V·∫Ω bi·ªÉu ƒë·ªì Feature Importance & D·ª± b√°o
            viz.plot_all_features(pandas_df)
            viz.plot_metrics_comparison(metrics)
            print("   ‚úÖ Visualization charts generated.")
        except Exception as v_err:
            print(f"   ‚ö†Ô∏è Visualization failed (Non-critical): {v_err}")

    except Exception as e:
        print(f"\n‚ùå Error in pipeline: {e}")
        import traceback
        traceback.print_exc()
    finally:
        spark.stop()

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--city', type=str, default=None)
    parser.add_argument('--limit', type=int, default=None)
    parser.add_argument('--no-save', action='store_true')
    args = parser.parse_args()
    
    run_forecasting_pipeline(
        city=args.city, 
        limit_rows=args.limit,
        save_models=not args.no_save
    )