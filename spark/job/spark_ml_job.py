"""
Weather Forecasting - Main Pipeline
D·ª± ƒëo√°n th·ªùi ti·∫øt s·ª≠ d·ª•ng Spark ML v√† d·ªØ li·ªáu t·ª´ MinIO
"""

from pyspark.sql import SparkSession
import sys
import os
import argparse
from datetime import datetime

# --- IMPORT MODULES ---
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    import config
    from visualization import ForecastVisualizer
    from data_loader import WeatherDataLoader
    from feature_engineering import TimeSeriesFeatureEngineer
    from models import WeatherForecastModels
    from forecast_evaluator import ForecastEvaluator
    from postgres_writer import PostgresWriter  # <--- ƒê√£ th√™m Import n√†y
except ImportError as e:
    print(f"‚ùå L·ªói Import: {e}")
    print("üí° ƒê·∫£m b·∫£o b·∫°n ƒëang ch·∫°y file n√†y t·ª´ th∆∞ m·ª•c spark/job/ ho·∫∑c ƒë√£ setup PYTHONPATH ƒë√∫ng.")
    sys.exit(1)

# C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n l∆∞u Model/Output c·ª•c b·ªô
LOCAL_MODEL_DIR = "./models_output"
LOCAL_OUTPUT_DIR = "./predictions_output"
TRAIN_TEST_SPLIT = 0.8

def create_spark_session():
    """
    Kh·ªüi t·∫°o Spark Session v·ªõi c·∫•u h√¨nh MinIO S3A + PostgreSQL Driver
    """
    print("\n" + "="*80)
    print("üöÄ WEATHER FORECASTING ML SYSTEM")
    print("="*80)
    print("‚ö° Initializing Spark Session...")
    
    packages = [
        "org.apache.hadoop:hadoop-aws:3.3.4",
        "com.amazonaws:aws-java-sdk-bundle:1.12.262",
        "org.postgresql:postgresql:42.6.0" # <--- Driver Postgres
    ]
    
    builder = SparkSession.builder \
        .appName("WeatherForecast_Training") \
        .master("local[*]") \
        .config("spark.driver.memory", "2g") \
        .config("spark.executor.memory", "2g") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.jars.packages", ",".join(packages))
    
    # N·∫°p c·∫•u h√¨nh MinIO t·ª´ file config.py
    if hasattr(config, 'SPARK_S3_CONFIG'):
        for key, value in config.SPARK_S3_CONFIG.items():
            builder = builder.config(key, value)
    else:
        print("‚ö†Ô∏è  Warning: SPARK_S3_CONFIG not found in config.py")
    
    spark = builder.getOrCreate()
    spark.sparkContext.setLogLevel("WARN")
    
    print("‚úÖ Spark Session initialized successfully!")
    return spark


def run_forecasting_pipeline(city: str = None, limit_rows: int = None, save_models: bool = True):
    
    spark = create_spark_session()
    
    try:
        # ==========================================
        # STEP 1: LOAD DATA FROM MINIO
        # ==========================================
        print("\n" + "="*80)
        print("STEP 1: LOADING DATA FROM MINIO")
        print("="*80)
        
        loader = WeatherDataLoader(spark)
        df = loader.load_data(city=city, limit_rows=limit_rows)
        
        # Validate data
        validation = loader.validate_data(df)
        if validation['quality_score'] < 50:
            print("‚ö†Ô∏è Data quality too poor. Exiting.")
            return

        loader.summary_stats(df)
        
        # ==========================================
        # STEP 2: FEATURE ENGINEERING
        # ==========================================
        print("\n" + "="*80)
        print("STEP 2: FEATURE ENGINEERING")
        print("="*80)
        
        engineer = TimeSeriesFeatureEngineer()
        df_features = engineer.engineer_all_features(df)
        
        feature_cols = engineer.get_feature_columns(df_features, exclude_targets=True)
        print(f"\nüìä Total features created: {len(feature_cols)}")
        
        # ==========================================
        # STEP 3: TRAIN/TEST SPLIT
        # ==========================================
        print("\n" + "="*80)
        print("STEP 3: SPLITTING DATA")
        print("="*80)
        
        # X√≥a d√≤ng null (do lag feature t·∫°o ra)
        df_clean = df_features.dropna()
        
        # Split 80/20
        train_df, test_df = df_clean.randomSplit([TRAIN_TEST_SPLIT, 1 - TRAIN_TEST_SPLIT], seed=42)
        
        print(f"Training set:   {train_df.count()} rows")
        print(f"Test set:       {test_df.count()} rows")
        
        if train_df.count() < 50:
            print("‚ùå Not enough data to train. Need at least 50 rows.")
            return

        # ==========================================
        # STEP 4 & 5: BUILD & TRAIN MODELS
        # ==========================================
        print("\n" + "="*80)
        print("STEP 4 & 5: BUILDING & TRAINING MODELS")
        print("="*80)
        
        model_builder = WeatherForecastModels()
        
        # 1. Build Pipelines
        model_builder.build_all_models(feature_cols)
        
        # 2. Train
        trained_models = model_builder.train_all_models(train_df)
        
        if save_models:
            print(f"\nüíæ Saving models to {LOCAL_MODEL_DIR}...")
            if not os.path.exists(LOCAL_MODEL_DIR):
                os.makedirs(LOCAL_MODEL_DIR)
            model_builder.save_all_models(trained_models, LOCAL_MODEL_DIR)
        
        # ==========================================
        # STEP 6: EVALUATE & PREDICT
        # ==========================================
        print("\n" + "="*80)
        print("STEP 6: PREDICTION & EVALUATION")
        print("="*80)
        
        predictions_df = test_df
        # Th·ª±c hi·ªán d·ª± ƒëo√°n cho t·∫•t c·∫£ c√°c target
        for target, model in trained_models.items():
            predictions_df = model.transform(predictions_df)
            
        evaluator = ForecastEvaluator()
        metrics = evaluator.evaluate_all_models(predictions_df)
        
        print("\nüìä Evaluation Summary:")
        for target, m in metrics.items():
            print(f"   - {target}: RMSE={m.get('rmse', 'N/A'):.4f}, R2={m.get('r2', 'N/A'):.4f}")

        # ==========================================
        # STEP 7: WRITE TO POSTGRESQL 
        # ==========================================
        print("\n" + "="*80)
        print("STEP 7: WRITING TO POSTGRESQL")
        print("="*80)

        # 1. Ch·ªçn l·ªçc c√°c c·ªôt c·∫ßn thi·∫øt ƒë·ªÉ ghi v√†o DB
        # Ch√∫ng ta KH√îNG ghi c√°c feature lag/rolling, ch·ªâ ghi: Time, City, Actual, Prediction
        target_cols = list(config.CONTINUOUS_FEATURES) 
        
        if hasattr(config, 'CATEGORICAL_FEATURES'):
            target_cols += config.CATEGORICAL_FEATURES            
        prediction_cols = [f"prediction_{c}" for c in target_cols]
        
        # T·∫°o danh s√°ch c·ªôt c·∫ßn select
        select_cols = ['datetime', 'city'] 
        select_cols += [c for c in target_cols if c in predictions_df.columns] # Gi√° tr·ªã th·ª±c
        select_cols += [c for c in prediction_cols if c in predictions_df.columns] # Gi√° tr·ªã d·ª± ƒëo√°n
        
        print(f"   Selecting {len(select_cols)} columns for database...")
        export_df = predictions_df.select(select_cols)
        
        # 2. G·ªçi Postgres Writer
        pg_writer = PostgresWriter()
        success = pg_writer.write_predictions_safe(export_df)
        
        if success:
            print("   ‚úÖ Database update complete.")
        else:
            print("   ‚ö†Ô∏è Database update skipped/failed.")

        # ==========================================
        # STEP 8: EXPORT CSV (Local Backup)
        # ==========================================
        if not os.path.exists(LOCAL_OUTPUT_DIR):
            os.makedirs(LOCAL_OUTPUT_DIR)
            
        output_file = os.path.join(LOCAL_OUTPUT_DIR, f"forecast_{datetime.now().strftime('%Y%m%d')}.csv")
        print(f"\n‚úÖ Pipeline Complete. (Metrics & Models saved)")
        # 2. Th√™m ƒëo·∫°n n√†y tr∆∞·ªõc khi k·∫øt th√∫c
        print("\n" + "="*80)
        print("STEP 8: VISUALIZATION")
        print("="*80)

        viz = ForecastVisualizer()
        # Chuy·ªÉn Spark DataFrame sang Pandas ƒë·ªÉ v·∫Ω (L∆∞u √Ω: Ch·ªâ l√†m khi d·ªØ li·ªáu test nh·ªè < 100k d√≤ng)
        pandas_df = predictions_df.toPandas()

        viz.plot_all_features(pandas_df)
        viz.plot_metrics_comparison(metrics) # 'metrics' l·∫•y t·ª´ b∆∞·ªõc Evaluator
    except Exception as e:
        print(f"\n‚ùå Error in pipeline: {e}")
        import traceback
        traceback.print_exc()
    finally:
        spark.stop()

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--city', type=str, default=None)
    parser.add_argument('--limit', type=int, default=None)
    parser.add_argument('--no-save', action='store_true')
    args = parser.parse_args()
    
    run_forecasting_pipeline(
        city=args.city, 
        limit_rows=args.limit,
        save_models=not args.no_save
    )