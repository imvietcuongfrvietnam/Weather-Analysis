"""
Forecast Evaluator - Metrics and Evaluation for Weather Forecasting
ƒê√°nh gi√° ƒë·ªô ch√≠nh x√°c c·ªßa c√°c m√¥ h√¨nh d·ª± ƒëo√°n
Updated: Optimized for Spark ML 3.x
"""

from pyspark.sql import DataFrame
from pyspark.sql.functions import col, abs as spark_abs, mean
from pyspark.ml.evaluation import RegressionEvaluator, MulticlassClassificationEvaluator
import sys
import os

# Setup import config
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    import config
except ImportError:
    # Fallback config
    class Config:
        CONTINUOUS_FEATURES = ["temperature", "humidity", "pressure", "wind_speed", "precipitation_mm"]
        CATEGORICAL_FEATURES = ["weather_desc"]
    config = Config()

class ForecastEvaluator:
    """Evaluate forecast model performance"""
    
    @staticmethod
    def evaluate_regression(predictions_df: DataFrame, target_feature: str) -> dict:
        """
        ƒê√°nh gi√° m√¥ h√¨nh h·ªìi quy (Regression)
        Metrics: MAE, RMSE, R2, MAPE
        """
        prediction_col = f"prediction_{target_feature}"
        
        # Ki·ªÉm tra c·ªôt t·ªìn t·∫°i
        if prediction_col not in predictions_df.columns or target_feature not in predictions_df.columns:
            print(f"   ‚ö†Ô∏è  Skipping {target_feature}: Prediction or Target column not found.")
            return {}
        
        # L·ªçc b·ªè gi√° tr·ªã null ƒë·ªÉ tr√°nh l·ªói t√≠nh to√°n
        eval_df = predictions_df.select(target_feature, prediction_col).dropna()
        
        count = eval_df.count()
        if count == 0:
            print(f"   ‚ö†Ô∏è  Skipping {target_feature}: No valid rows to evaluate.")
            return {}

        metrics = {}
        
        # 1. RMSE (Root Mean Squared Error)
        rmse_evaluator = RegressionEvaluator(
            labelCol=target_feature, predictionCol=prediction_col, metricName="rmse"
        )
        metrics['rmse'] = rmse_evaluator.evaluate(eval_df)
        
        # 2. MAE (Mean Absolute Error)
        mae_evaluator = RegressionEvaluator(
            labelCol=target_feature, predictionCol=prediction_col, metricName="mae"
        )
        metrics['mae'] = mae_evaluator.evaluate(eval_df)
        
        # 3. R2 (R-squared)
        r2_evaluator = RegressionEvaluator(
            labelCol=target_feature, predictionCol=prediction_col, metricName="r2"
        )
        metrics['r2'] = r2_evaluator.evaluate(eval_df)
        
        # 4. MAPE (Mean Absolute Percentage Error) - T√≠nh th·ªß c√¥ng v√¨ Spark c≈© kh√¥ng c√≥ s·∫µn
        # MAPE = mean( abs((actual - pred) / actual) ) * 100
        # Th√™m 0.001 v√†o m·∫´u s·ªë ƒë·ªÉ tr√°nh chia cho 0
        mape_df = eval_df.withColumn(
            "ape", 
            spark_abs((col(target_feature) - col(prediction_col)) / (spark_abs(col(target_feature)) + 0.001))
        )
        metrics['mape'] = mape_df.agg(mean("ape")).collect()[0][0] * 100
        
        metrics['sample_count'] = count
        
        return metrics
    
    @staticmethod
    def evaluate_classification(predictions_df: DataFrame, target_feature: str) -> dict:
        """
        ƒê√°nh gi√° m√¥ h√¨nh ph√¢n lo·∫°i (Classification)
        Metrics: Accuracy, F1, Precision, Recall
        """
        # C·ªôt d·ª± ƒëo√°n (d·∫°ng s·ªë index)
        prediction_col = f"prediction_{target_feature}"
        # C·ªôt label th·ª±c t·∫ø (d·∫°ng s·ªë index - do StringIndexer t·∫°o ra)
        label_col = f"{target_feature}_index"
        
        if prediction_col not in predictions_df.columns or label_col not in predictions_df.columns:
            print(f"   ‚ö†Ô∏è  Skipping {target_feature}: Columns not found ({label_col}, {prediction_col})")
            return {}
        
        eval_df = predictions_df.select(label_col, prediction_col).dropna()
        count = eval_df.count()
        
        if count == 0:
            return {}
            
        metrics = {}
        
        # Helper function ƒë·ªÉ t·∫°o evaluator
        def get_evaluator(metric_name):
            return MulticlassClassificationEvaluator(
                labelCol=label_col,
                predictionCol=prediction_col,
                metricName=metric_name
            )

        metrics['accuracy'] = get_evaluator("accuracy").evaluate(eval_df)
        metrics['f1'] = get_evaluator("f1").evaluate(eval_df)
        metrics['precision'] = get_evaluator("weightedPrecision").evaluate(eval_df)
        metrics['recall'] = get_evaluator("weightedRecall").evaluate(eval_df)
        metrics['sample_count'] = count
        
        return metrics
    
    @staticmethod
    def evaluate_all_models(predictions_df: DataFrame) -> dict:
        """
        ƒê√°nh gi√° to√†n b·ªô c√°c m√¥ h√¨nh (c·∫£ Regression v√† Classification)
        """
        print("\n" + "="*60)
        print("üìä EVALUATING MODEL PERFORMANCE")
        print("="*60)
        
        all_metrics = {}
        
        # 1. Evaluate Regression Models
        print("\nüî¢ Regression Models:")
        for target in config.CONTINUOUS_FEATURES:
            # Ch·ªâ ƒë√°nh gi√° n·∫øu c√≥ c·ªôt d·ª± ƒëo√°n trong DataFrame
            if f"prediction_{target}" in predictions_df.columns:
                print(f"   Evaluating {target}...")
                metrics = ForecastEvaluator.evaluate_regression(predictions_df, target)
                if metrics:
                    all_metrics[target] = metrics
                    print(f"      RMSE: {metrics['rmse']:.4f}, R2: {metrics['r2']:.4f}")
        
        # 2. Evaluate Classification Models
        # (N·∫øu b·∫°n ch∆∞a implement classification th√¨ ph·∫ßn n√†y s·∫Ω skip)
        if hasattr(config, 'CATEGORICAL_FEATURES'):
            print("\nüè∑Ô∏è  Classification Models:")
            for target in config.CATEGORICAL_FEATURES:
                if f"prediction_{target}" in predictions_df.columns:
                    print(f"   Evaluating {target}...")
                    metrics = ForecastEvaluator.evaluate_classification(predictions_df, target)
                    if metrics:
                        all_metrics[target] = metrics
                        print(f"      Accuracy: {metrics['accuracy']:.4f}, F1: {metrics['f1']:.4f}")
        
        print("="*60 + "\n")
        return all_metrics

if __name__ == "__main__":
    print("Forecast Evaluator Module Loaded.")