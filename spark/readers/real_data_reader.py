from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import from_json, col
from pyspark.sql.types import StructType
import sys
import os

# --- IMPORT CONFIG & SCHEMA ---
# VÃ¬ main_etl.py Ä‘Ã£ setup sys.path, ta cÃ³ thá»ƒ import trá»±c tiáº¿p
try:
    from config import kafka_config
    # Giáº£ Ä‘á»‹nh báº¡n Ä‘á»ƒ schema á»Ÿ spark/schemas/weather_schema.py
    # Náº¿u tÃªn file khÃ¡c, hÃ£y sá»­a láº¡i dÃ²ng nÃ y
    from schemas.weather_schema import weather_schema 
except ImportError as e:
    print(f"âŒ Lá»—i Import trong DataReader: {e}")
    # Fallback schema cÆ¡ báº£n náº¿u khÃ´ng import Ä‘Æ°á»£c (Ä‘á»ƒ trÃ¡nh crash app ngay láº­p tá»©c)
    from pyspark.sql.types import StringType, StructField
    weather_schema = StructType([StructField("error", StringType(), True)])

class DataReader:
    """
    Class Ä‘á»c dá»¯ liá»‡u chuyÃªn dá»¥ng cho Weather Project.
    Há»— trá»£ Ä‘á»c tá»«:
    1. Kafka (Production - Streaming)
    2. JSON/CSV Local (Testing)
    """
    
    def __init__(self, spark: SparkSession, source_type: str = "kafka", kafka_mode: str = "streaming"):
        """
        Args:
            spark: SparkSession object
            source_type: "kafka" hoáº·c "file"
            kafka_mode: "streaming" hoáº·c "batch"
        """
        self.spark = spark
        self.source_type = source_type
        self.kafka_mode = kafka_mode
        
        # ÄÆ°á»ng dáº«n data test cá»¥c bá»™ (náº¿u cáº§n)
        self.local_data_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), "data")
        
        print(f"ğŸ“Š DataReader initialized | Source: {source_type.upper()} | Mode: {kafka_mode.upper()}")
    
    def read_weather_data(self) -> DataFrame:
        """
        HÃ m chÃ­nh Ä‘Æ°á»£c gá»i bá»Ÿi main_etl.py
        """
        if self.source_type == "kafka":
            # Láº¥y tÃªn topic tá»« file config
            topic = kafka_config.KAFKA_TOPICS.get('weather', 'weather_data')
            return self._read_from_kafka(topic, weather_schema)
            
        elif self.source_type == "file":
            return self._read_from_file("weather_data.csv", weather_schema)
            
        else:
            raise ValueError(f"âŒ Unknown source type: {self.source_type}")

    # ============================================
    # Private Methods
    # ============================================
    
    def _read_from_kafka(self, topic: str, schema: StructType) -> DataFrame:
        """
        Äá»c dá»¯ liá»‡u tá»« Kafka vÃ  Parse JSON struct
        """
        print(f"ğŸ“¡ Connecting to Kafka servers: {kafka_config.KAFKA_BOOTSTRAP_SERVERS}")
        print(f"ğŸ“¥ Subscribing to topic: {topic}")

        # 1. Cáº¥u hÃ¬nh Kafka Reader
        if self.kafka_mode == "streaming":
            df_reader = self.spark.readStream \
                .format("kafka") \
                .option("kafka.bootstrap.servers", kafka_config.KAFKA_BOOTSTRAP_SERVERS) \
                .option("subscribe", topic) \
                .option("startingOffsets", "earliest") \
                .option("failOnDataLoss", "false")
        else:
            # Batch mode (cho debug)
            df_reader = self.spark.read \
                .format("kafka") \
                .option("kafka.bootstrap.servers", kafka_config.KAFKA_BOOTSTRAP_SERVERS) \
                .option("subscribe", topic)
        
        # 2. Load dá»¯ liá»‡u Raw (Binary)
        df = df_reader.load()
        
        # 3. Parse Data: Binary -> String -> JSON Struct
        # Kafka tráº£ vá» cá»™t 'value' dáº¡ng binary
        parsed_df = df.selectExpr("CAST(value AS STRING) as json_string") \
            .select(from_json(col("json_string"), schema).alias("data")) \
            .select("data.*") # Flatten struct ra cÃ¡c cá»™t
            
        print("âœ… Kafka Stream initialized successfully.")
        return parsed_df

    def _read_from_file(self, filename: str, schema: StructType) -> DataFrame:
        """
        Äá»c file CSV/JSON local Ä‘á»ƒ test logic mÃ  khÃ´ng cáº§n báº­t Kafka
        """
        file_path = os.path.join(self.local_data_path, filename)
        print(f"ğŸ“‚ Reading local file: {file_path}")
        
        if filename.endswith(".csv"):
            return self.spark.read.csv(file_path, header=True, schema=schema)
        elif filename.endswith(".json"):
            return self.spark.read.json(file_path, schema=schema)
        else:
            # Máº·c Ä‘á»‹nh thá»­ Ä‘á»c parquet
            return self.spark.read.parquet(file_path)