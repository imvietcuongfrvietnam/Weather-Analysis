from pyspark.sql import SparkSession, DataFrame
import os
import sys

# Import trá»±c tiáº¿p vÃ¬ file náº±m cÃ¹ng folder job hoáº·c Ä‘Ã£ Ä‘Æ°á»£c thÃªm vÃ o sys.path
try:
    import minio_config
except ImportError:
    from config import minio_config

class MinIOWriter:
    """
    Class chuyÃªn trÃ¡ch ghi dá»¯ liá»‡u xuá»‘ng MinIO (Data Lake)
    Sá»­ dá»¥ng giao thá»©c S3A cá»§a Spark.
    """
    
    def __init__(self):
        self.bucket = minio_config.MINIO_BUCKET
        print(f"ğŸ“¦ MinIO Writer initialized for bucket: {self.bucket}")

    def write_stream(self, df: DataFrame, folder: str = "enriched", trigger_time="30 seconds"):
        """
        Ghi luá»“ng dá»¯ liá»‡u (Streaming) xuá»‘ng MinIO Ä‘á»‹nh dáº¡ng Parquet.
        
        Args:
            df: DataFrame cáº§n ghi
            folder: TÃªn folder lÆ°u trá»¯ (vd: 'enriched', 'cleaned')
            trigger_time: Chu ká»³ ghi (vd: '30 seconds', '1 minute')
        """
        # 1. Táº¡o Ä‘Æ°á»ng dáº«n file (Path)
        output_path = f"s3a://{self.bucket}/{folder}/weather"
        
        # 2. Táº¡o Ä‘Æ°á»ng dáº«n Checkpoint
        # Sá»­ dá»¥ng /tmp Ä‘á»ƒ Ä‘áº£m báº£o quyá»n ghi trong mÃ´i trÆ°á»ng K8s náº¿u S3A checkpoint bá»‹ lá»—i
        checkpoint_path = f"/tmp/checkpoints/weather_{folder}"
        
        print(f"\nğŸ’¾ [MinIO] Config Streaming Write:")
        print(f"   ğŸ‘‰ Output:     {output_path}")
        print(f"   ğŸ‘‰ Checkpoint: {checkpoint_path}")
        
        # 3. Khá»Ÿi táº¡o Query Streaming
        query = df.writeStream \
            .outputMode("append") \
            .format("parquet") \
            .option("path", output_path) \
            .option("checkpointLocation", checkpoint_path) \
            .trigger(processingTime=trigger_time) \
            .queryName(f"WriteToMinIO_{folder}") \
            .start()
            
        return query